<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Surely You're Joking, Mr. Feynman! | Mahesh Sathiamoorthy</title>

    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Surely You’re Joking, Mr. Feynman! | Mahesh Sathiamoorthy</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Surely You’re Joking, Mr. Feynman!" />
<meta name="author" content="mahesh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I recently finished this book (should have read it long back!) and I must say, what a great read about a great and a curious mind!" />
<meta property="og:description" content="I recently finished this book (should have read it long back!) and I must say, what a great read about a great and a curious mind!" />
<link rel="canonical" href="http://localhost:4000/joking-feynman/" />
<meta property="og:url" content="http://localhost:4000/joking-feynman/" />
<meta property="og:site_name" content="Mahesh Sathiamoorthy" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-01-04T20:30:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Surely You’re Joking, Mr. Feynman!" />
<script type="application/ld+json">
{"headline":"Surely You’re Joking, Mr. Feynman!","dateModified":"2018-01-04T20:30:00-08:00","url":"http://localhost:4000/joking-feynman/","datePublished":"2018-01-04T20:30:00-08:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/joking-feynman/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"},"name":"mahesh"},"author":{"@type":"Person","name":"mahesh"},"description":"I recently finished this book (should have read it long back!) and I must say, what a great read about a great and a curious mind!","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon-16x16.png">
    <link rel="manifest" href="/assets/images/site.webmanifest">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>

    <!-- LaTeX -->
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-7617395-1', 'auto');
  ga('send', 'pageview');

</script>


</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>Mahesh Sathiamoorthy</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/">Home</a>
</li>

<li class="nav-item">
<a class="nav-link" href="/tech">Technology</a>
</li>

<li class="nav-item">
<a class="nav-link" href="/mind-over-matter">Mind Over Matter</a>
</li>

<li class="nav-item">
<a class="nav-link" href="/blog">Blog</a>
</li>

<li class="nav-item">
<a class="nav-link" href="http://photos.smahesh.com">Photography</a>
</li>


<li class="nav-item">
<a class="nav-link" href="/about">About</a>
</li>

            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about",
    "title": "About",
    "body": "Who am I?: I am Maheswaran Sathiamoorthy, but go by Mahesh Sathiamoorthy. I am a Software Engineer at Google since 2014 and work on ML/Recommender Systems/TPU (since 2017). Please note that the opinions here are my own and not of my employer. From 2008 to 2013, I was a PhD student at Ming Hsieh Department of Electrical Engineering, University of Southern California. I worked with Prof. Bhaskar Krishnamachariand Prof. Alex Dimakis. My thesis was titled “Optimizing distributed storage in cloud environments”. Before that, I did my B. Tech(H) in Electronics and Electrical Communication Engineering at the Indian Institute of Technology Kharagpur, India, from 2004 to 2008. What is this site about?: This is my home page and blog. I am interested and write/post about the following:  Technology (tutorials, commentary).  About cognition, our mind, sharpening it with mental models, cognitive biases.  Photography and art (coming someday!).  and of course miscellaneous articles, in the form of blogs. This site was made with Mundana Jekyll Theme by WowThemes. "
    }, {
    "id": 2,
    "url": "http://localhost:4000/blog.html",
    "title": "Blog",
    "body": "                        {{page. title}}:                 {{ page. intro }}                     Posts:       {% assign posts = site. posts | where: categories , blog  %}      {% for post in posts %}      {% include main-loop-card. html %}      {% endfor %}  "
    }, {
    "id": 3,
    "url": "http://localhost:4000/categories.html",
    "title": "Categories",
    "body": "          Categories          {% for category in site. categories %}     {{ category[0] }}:           {% assign pages_list = category[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 4,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "Contact me at mahesh at smahesh dot com. "
    }, {
    "id": 5,
    "url": "http://localhost:4000/",
    "title": "Technology, Photography, and the Mind",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            &lt;div class= col-md-6 d-none d-md-block pr-0  style= background-size:cover;background-image:url({% if post. image contains  ://  %}{{ post. image }}{% else %}{{ site. baseurl }}/{{ post. image }}{% endif %}); &gt;	            {% endif %}{% endfor %} {% endif %}             All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      &lt;/div&gt; "
    }, {
    "id": 6,
    "url": "http://localhost:4000/mind-over-matter.html",
    "title": "Mind over Matter",
    "body": "                        {{page. title}}:                  How to think clearly and effectively? What are the role of mental models in making sense of our complex surroundings? Why are some people able to do things effortlessly while others struggle?          These are some of the questions that I often ask myself. And I am endlessly fascinated by the human mind, the way it works, it's potential, and all the troubles it leads us to. Please join me in the journey to explore the gift we all have.                     Articles:       {% assign posts = site. posts | where: categories , mind over matter  %}      {% for post in posts %}      {% include main-loop-card. html %}      {% endfor %}  "
    }, {
    "id": 7,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 8,
    "url": "http://localhost:4000/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 9,
    "url": "http://localhost:4000/tech.html",
    "title": "Technology",
    "body": "                        {{page. title}}:                 {{ page. intro }}                     Articles:       {% assign posts = site. posts | where: categories , tech  %}      {% for post in posts %}      {% include main-loop-card. html %}      {% endfor %}  "
    }, {
    "id": 10,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 11,
    "url": "http://localhost:4000/page2/",
    "title": "Technology, Photography, and the Mind",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            &lt;div class= col-md-6 d-none d-md-block pr-0  style= background-size:cover;background-image:url({% if post. image contains  ://  %}{{ post. image }}{% else %}{{ site. baseurl }}/{{ post. image }}{% endif %}); &gt;	            {% endif %}{% endfor %} {% endif %}             All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      &lt;/div&gt; "
    }, {
    "id": 12,
    "url": "http://localhost:4000/page3/",
    "title": "Technology, Photography, and the Mind",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            &lt;div class= col-md-6 d-none d-md-block pr-0  style= background-size:cover;background-image:url({% if post. image contains  ://  %}{{ post. image }}{% else %}{{ site. baseurl }}/{{ post. image }}{% endif %}); &gt;	            {% endif %}{% endfor %} {% endif %}             All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      &lt;/div&gt; "
    }, {
    "id": 13,
    "url": "http://localhost:4000/2020-its-a-wrap/",
    "title": "2020: It's a wrap!",
    "body": "2020/12/29 - Looking back at my blog, I am disappointed that my last post was in 2018. In fact, my 2018 resolution was to write 50k public words, but I barely got to a few thousand. 2019 seems to have come and gone, and thanks to a break at work, I have managed to get back to my blog a few days before 2021 knocks on the door. I do have some good reasons why I didn’t get to my blog all this time (becoming a dad, getting promoted at work, traveling quite a bit, and of course, COVID), but I think the biggest reason is the setup of the blog. This site and blog are designed and built using Jekyll. This means that if I have to write a post, I need to run a few commands and get them pushed to Github. That’s not that bad, but the other reason is thatall of the underlying technologies are installed in my desktop PC, and due to extensive traveling, I was mostly away from the desktop. I love how customizable Jekyll is, but I hate the overhead it has and hate having to debug build errors. If you want to pick up a new hobby, you always have to cut down on hindrances and make it as easy as it is possible to get to it. In my case, the overhead (which wasn’t really that bad, in retrospect) seems to have stopped me from getting to edit posts. In fact, early on, I was using Octopress and in between it broke down, leaving a bad taste in my mouth. All of these, combined with the other complexities of life, meant that I didn’t write as much as I should have written. But thankfully, I did do something else as much as possible in 2019 and 2020: to read as many books as possible. I managed to finish 22 books in 2019 and 32 in 2020 (so almost two books per month). Thanks to the year end break, a few days back I took a second look at my site and was appalled at the state of affairs: the homepage was severely outdated (still read as if I was at USC doing my PhD). My homepage lived in a separate repository in Github, and the blog itself was a different repository. They both had different themes, though they maintained the same domain name. I still do have a different domain for the photos I take (which is also sitting unattended for a couple of years now) and hope to bring those into this domain as well. I looked into moving over to wordpress so as to make it easy to create, edit and publish posts (remember, no friction), but I still wasn’t happy with the amount of customizability it gave. Fortunately, I stumbled across a different template I liked, and was able to customize it to my liking. I have designed the site such that there are three category of articles: stuff about technology, which includes tutorials, commentaries etc. , and stuff about the mind and cognitition, which I am titling as Mind over Matter, and lastly miscellaneous articles such as this in the form of blogs. "
    }, {
    "id": 14,
    "url": "http://localhost:4000/finding-passion/",
    "title": "Finding Passion",
    "body": "2018/07/11 - One would think that Jeff Bezos’s passion has always been Amazon and online retail, but turns out his passions areRockets, space travel and propulsion.  So he did start out doing what he was NOT passionate about, and was able to make more than a living. I am pretty sure he became passionate about Amazon possibly without actively realizing it, otherwise its unlikely Amazon will be at the place where it is today. Bezos thinks that passion finds you, instead of the other way around. While I think may be he is right about the first half, I think one can still find a passion and become passionate about something that they may not even like to begin with. I think that one can become passionate about anything that they are working on, as long as it is not too boring and allows one to slowly increase expertise in that line of work. You may not be able to become passionate about flipping burgers, but may be able to become passionate about software engineering/machine learning/carpentry or anything else for that matter. Let’s look at Bezos’s passion of rockets. Most likely, he was not born with a rocket-gene. He probably became interested in rockets because he either read about rockets, or saw something about rockets on the TV or in real life. He may have seen many things, but why did he become so interested in rockets? There can be any number of reasons here. May be someone influential (that can include father/mother/older sibling) was instrumental, or may be he was exposed to a few other things that made it easy for him to understand about rockets easily. Plus, in the case of rockets, they already bring with them a wow effect that makes it easy for anyone to get captivated with them. Sure, his circumstances made it so that the passion of rockets chose him. But the point is that his passion didn’t come out of nowhere, but actually was something that he developed, without realizing it. Similarly, it should be possible to develop new passions. You will have to start at the bottom, start exploring the space of the domain that you want to become passionate about, so that you are able to develop good mental representations of the domain and understand the challenges in the domain. It is a slow difficult process, but as you get deeper and deeper into a subject, you will realize that you are more and more interested in it, and will slowly start to develop passion for it. "
    }, {
    "id": 15,
    "url": "http://localhost:4000/deliberate-practice/",
    "title": "Deliberate Practice",
    "body": "2018/05/28 - I read the book Outliers several years ago, and as far as I can remember, I was very impressed with the book. Then I started hearing complaints about the book: that the ten thousand hour rule that the author, Malcom Gladwell, had been preaching, was over-simplified, and Gladwell may have not stuck to high standards when reporting on some examples. In case you have not read that book, and have no idea what I am talking about, here is the gist of ten thousand hours rule: if you want to become a world-class expert in a domain, you will need to practice for around ten thousand hours or so. Or at least that’s the message that I remember from the book. The problem is that, this message is not accurate, if not misleading. First, there is no magic number of hours (“ten thousand”) that will buy you greatness and glory, and you cannot just “practice” your way to expertise. Here is an example of a random goodreads review for Outliers that exemplifies the problem with the book:  He states that it takes approximately 10,000 hours to master something and that gives me comfort.  “2 hours down, only 9,998 left to go. ” Ouch. The ten thousand hour rule was derived from the work by Anders Ericsson, somebody whose job is to study experts, expert performance, and what leads to expertise. After the publication of the Outliers book, Anders started popping up on my radar and was complaining about the book (for example, I heard him talk on Freakonomics). So when I learnt that Anders had published a book on this subject, called Peak, I was very curious to take a look. I am still reading the book, but I am glad I picked it up — this book covers a lot of ground and goes the whole nine yards about expertise. Running the risk of over-simplification, if there is one main take-away from the book, it is that “deliberate practice” and not just the regular old “practice” that is essential for expertise. Note that shortly after finishing reading Outliers, I did come across this concept of deliberate practice and understood the subject, but this book has been useful in filling in gaps and connecting a lot of dots that I have been thinking about expertise. Let’s talk about deliberate practice: Deliberate practice is very different from regular practice. Most of us have been typing for a long time (i. e. practicing the skill of “typing”). I might have even practiced this skill for ten thousand hours by now. Am I an expert typist? Heck no. So what happened? I was just practicing it without focusing on improving the skill. It was not mindful, it was very much mindless. This sort of mindful practice is what is Ericsson calls as Deliberate practice. You have to intensely focus on what you are doing with a goal to improve. In short, after you come out of a deliberate practice session, your head should hurt (of course I am exaggerating here a bit, but my point is that deliberate practice is very difficult and is not a pleasant activity). Ericsson gives similar examples like typing: you don’t know how to play tennis, you start practicing and you slowly get better until one day you realize that you have hit a plateau and have not improved for a long time now. Most likely this is because initially you had to focus quite a bit to get to a decent shape, and then you have stopped focusing, and the games are generally fun. Mostly you are having nice conversations in the court with your opponent. This is where focus comes in. You have to focus very carefully to learn from your mistakes. Talking about the American swimmer Natalie Coughlin, Ericsson writes:  While she was a good swimmer, she didn’t become great until she learned to focus throughout her practice. This struck a chord with me, since, unfortunately, I realized that I have spent a lot of hours in the gym without really focusing on what I was doing. So, how exactly does deliberate practice help you become an expert? The answer lies in mental representations. Mental representations: Consider reading: when you started out reading (as a child, I suppose), you would read character by character, and then read out the word. As time went by, you were able to “internalize” words and were able to read out word by word, instead of character by character. What happened here is that your mental represenation of language improved. When you look at a word, you immediately know what the word is, instead of reading it out as i-m-m-e-d-i-a-t-e-l-y. Similary, experts when compared to others, have much better mental representations in their domains. For example, Chess players parse the board much differently than the rest of us. When most of us look at the board, we see the individual pieces and the grid and get a rough idea about what the pieces are doing. An absolute new beginner who hasn’t seen a chess board before will not even be able to grasp as much information as you do. Expert chess players, on the other hand, absorb in a lot more information — they see which pieces are weakly positioned vs. which ones are strongly positioned. They probably almost see a story that is playing out on the board. If you show a valid board, and then ask them to recall, they are able to recall very well. I, on the other hand, will struggle to recall a few pieces here and there. Also, interestingly, if you show an invalid board, with the pieces placed incorrectly, they will struggle to recall the board! The point about mental represntations is that better mental representations help you think faster and clearer than others. And deliberate practice helps to hone the mental representations. There are of course a number of other factors that help and influence expertise (for example, it helps a lot to have great teachers and to get immediate feedback so as to correct your mistakes), and Ericsson does a great job about explaining all of this. If you are interested in delving deeper into this subject, I highly recommend the book! "
    }, {
    "id": 16,
    "url": "http://localhost:4000/dark-knowledge-and-distillation/",
    "title": "Dark Knowledge and distillation",
    "body": "2018/05/27 - Guess what this image contains? Most of you will think this is a dog, but a handful may think this is a wolf, and certainly no human readers will guess the above image to contain a car. Similarly, if I were a well trained neural network who has never seen this image before, I will assign a very high probability for this being a dog, may be a reasonaly low probability for this being a wolf and extremely low probability for this being a car. Now, hold on to this thought. We will come back to this later but first let’s talk about ensembling to improve the performance of machine learning models. Ensembling: One way to boost the performance (in terms of accuracy/loss) of a ML system is to ensemble multiple learners instead of a single one. If you are wondering that this is easy and there must be some catch, the catch is that this becomes very costly during serving. By serving, I mean using the trained model to run predictions in real world. For example, you have trained an image classifier, and now you want to use it to classify images uploaded from your app. In the absence of ensembling you had to invoke one model, but now with ensembling, you have to compute the predictions across many models and then aggregate them. This will add to your CPU cost, to your memory cost, as well as the latency, and so, for many production systems, ensembling remains a difficult choice. This where model compression and distillation come in. Model compression: Model compression was introduced in KDD 2006 by paper appropriately titled as “Model compression”. The goal is to compress large models into smaller models, so as to reduce the serving costs. At that time, large models generally referred to tree based models (e. g. random forest that contain several trees). What a different world it was back then! Here is a relevant sentence from the paper that summarizes it all:  Compression works by labeling a large unlabeled data set with the target model, and then training a neural net using the newly labeled data    Bucila et. al.  The authors then talk about how to generate this large unlabeled dataset using a few techniques. As an aside, note that this paper was written in 2006 when TPUs didn’t exist. Large tree based models may not be as well tuned to run on custom hardware as neural networks. They may incur a lot of cost when executing “if else” branching control flow statements, whereas with neural networks, we can replace all of these with very fast matrix multiplications. Distillation: Distillation is very similar to model compression and was introduced recently (2014-2015) by Geoff Hinton, father of Deep learning. Distillation refers to “distilling” the knowledge from one model to another, usually with the destination model being smaller than the source model. This sounds very much like model compression except that we want to exploit and learn from the “dark knowledge” that the bigger model has learnt. Dark Knowledge: Consider again the image classifier that you want to distill. You want to classify whether an image contains a cow, dog, cat, or a car and you have one big model that does this job fairly well (mostly better than humans), and you want to distill this into a smaller model. We have an image with a dog, and your model correctly predicts it to be a dog. If you are training your compressed model using this prediction, it is not very different from training against the targets. Instead you could look at the probabilities themselves. But most of these values are almost zero, and so they don’t affect the learning much either (you would have used a cross entropy loss to compare the probabilities of the bigger model against the probabilities of the smaller model). The “model compression” paper tries to overcome this problem by instead considering the logits (these are the values of the output layer that are fed to the softmax to get the probabilities). Herein lies the difference between this paper and Hinton’s distillation proposal. What Hinton does is to “soften” the probabilities to circumvent the above problem.  (taken from http://www. ttic. edu/dl/dark14. pdf) If you look at the second row (“output of geometric ensemble” which you consider as the output of the bigger model), you can notice that the probability for dog is pretty high. But hidden in that row is a lot more knowledge. You can see that the probability for cat is low but not as low as cow/car. This makes sense since there are cases when some dogs may look very similar to cats. Also, you can see that the probability of being a car is thousand times lower than the probability of being a cow. All this knowledge that the model has encoded is what Hinton calls as dark knowledge. So instead of just trying to transfer the knowledge that the image is of dog from a bigger to a smaller model, we want to transfer all this dark knowledge as well. Again, since we are dealing with very small numbers, we can soften these values using a temperature term: (taken from http://www. ttic. edu/dl/dark14. pdf) Now when you are training the smaller model, you compare against the soft targets to derive your loss. "
    }, {
    "id": 17,
    "url": "http://localhost:4000/joking-feynman/",
    "title": "Surely You're Joking, Mr. Feynman!",
    "body": "2018/01/04 - I recently finished this book (should have read it long back!) and I must say, what a great read about a great and a curious mind! Two things stood out for me:    How Richard Feynman was able to get into a new field and master it. He chronicles his journey of learning Portuguese (he ends up giving a technical talk entirely in Portuguese in Brazil), playing frigideira (he plays it in the Carnaval in Brazil), and learning painting (he was able to make several portraits and sell some paintings).     When he were to understand a new concept as a form of lecture, he would first ask for an example, and at the very beginning ask various questions. As he went along, he would keep working out the details of the problem against the example he got. This allowed him to understand the problem better when it starts to get complicated.  If you have not heard about the Feynman technique, check it out! "
    }, {
    "id": 18,
    "url": "http://localhost:4000/new-year-resolutions/",
    "title": "New Year's resolutions",
    "body": "2018/01/01 - 2018 is actually here (and that’s bad because it looks like time is crunching through years faster than I would like). And that also means it is time to set new year’s resolutions. Image above: Hiking up Koko crater trail in Oahu, Hawaii Setting realistic goals: For 2017, I had decided that I will do 250 miles of hiking. Given that there are 56 weeks in a year, this leads to a total of 250/56 = 4. 46 miles per week. But that should be pretty easy to hit, right? This is where most people make the mistake: they don’t take into account that life comes in between. There are weeks where you may be out of town, or you are sick, or there are a hundred other things that you need to take care of. So realistically, let’s say that I can go out for hikes for only 50% of the weeks. That means I will have to hit 9 miles every time I go for a hike. And since 9 mile hikes sounded reasonable, I decided that 250 miles might be a good goal to keep after all (I realized that I should have also set an elevation goal too, but thankfully my hiking friends made sure that I didn’t always go for low elevation hikes). In fact, this is kind of what happened. According to the spreadsheet I maintain, I did exactly 28 hikes in 2017 (27 hikes got me to the goal of 250 miles). Of course a lot of them were towards the end of the year. My friend had set a goal of 25 books to be read last year. How many did he actually do? According to Goodreads, only 4. Sure, he probably forgot to update, but it is very unlikely that he hit 25, especially given that the previous year also he had read less than 10 books. I did a similar mistake once by setting 18 books to read in 2015 and I barely managed to do 12. So for 2017, I had decided to set a smaller and more realistic goal (embarrasingly I didn’t hit my goal; I read only 10 books and didn’t want to count several of the other half read books). The point being that one shouldn’t set extremely unrealistic goals (for example, I knew that I can do 10 miles in a single hike, so my hiking goal felt realistic; and since in 2016 I might have done only 50 miles or so of hikes, 250 miles was definintely a worthy pursuit). In fact, I felt that setting goals based on number of books is problematic. I found that towards the end I was trying to search for small books so that I can bump up my numbers faster (and thus I read “Animal farm”). This is why I think instead of setting a goal on number of books that I have read, it is a better idea to set a goal of number of minutes to read. Put another way, it is meaningful to set a goal of reading half and hour everyday. I didn’t finish the 12 books that I wanted to read. But I did spend quite a bit of time doing a course (on Neural networks at the beginning of the year), and reading some papers (towards the end of the year; though I need to increase this a lot more), and reading other stuff. My resolution for 2018: I know for a fact that my communication skills can use a lot more improvement. While I still need to figure out what I am going to do to improve my verbal communication skill (meetups?), I have decided that I will write as much as possible to improve my written communication skill. So there you go. My 2018 resolution is going to be to write. But what good is a goal if it cannot be measured? I can either set a goal of say writing 1 hour every week or set a goal in terms of the number of words written. While, it is tempting to go with the former, I can totally see myself twiddling my thumb for an hour in the pretext of writing something. So I am going to decide a goal based on numbers. I am thinking of writing a total of 50,000 words publicly, and a total of 100,000 words privately (diary etc. ). This actually means writing roughly 1000 words publicly every week. "
    }, {
    "id": 19,
    "url": "http://localhost:4000/understanding-tensorflow-graph/",
    "title": "Understanding TensorFlow Graph Execution with a Simple Example",
    "body": "2017/07/10 - This post is for absolute beginners. I hope to be able to explain complicated concepts in simple terms to benefit a wider audience (see dummies guide to erasure coding post for a different example). Unfortunately though, you will need to know Python and numpy for understanding the code example below. TensorFlow: TensorFlow is a graph based processing framework that is really well suited for building Machine Learning models. I want to show a very simple example that involves TensorFlow’s Variables and Placeholders and illustrate how the graph execution works. Note that Tensorflow follows a deferred execution methodology: we initially set up a graph of how the computations should take place and later start the execution. Execution here refers to pumping in data continuously (in batches, more about that later) to the computation graph while tweaking the weights (and of course we generally have a goal in mind while tweaking the weights). Simple example: Let’s take a small example to make this idea concrete. Let’s say we have a bunch of (x, y) values that satisfy the formula y = ax + b. We know this list of numbers, know the formula, but do not know the values of a and b. The goal is to find out a and b. The first step is to define a graph. a and b are the weights that we want to compute. When an input x value is given, we want to compute ax + b, and so this requires a multiplication operator and an addition operator. Therefore we define our graph with these two operators. We can randomly assign some values to a and b, and this will give us a concrete value ax + b when a value of x is given to the graph during execution. We want this value to be equal to the corresponding y. So we can compare this value against the y value and use this to tweak the values of a and b. Gradient Descent is generally used to figure out how to modify these values, but for now, we can assume that TensorFlow knows how to do this. We just have to tell it what our ‘loss’ is (some function to compare how worse our estimate ax+b is from y), and we can pick an ‘optimizer’ that does gradient descent. One example of loss would be do take the difference between y and ax+b, and square it. Strictly speaking, the loss function and the optimizer are picked beforehand when we define the graph. Coming back to Variables and Placeholders: In TensorFlow parlance, tf. Variable is used to represent trainable variable, such as weights in a Neural Network. tf. Placeholder is used to represent variables that will be used to feed data to the graph. Let’s see the above idea in action. import tensorflow as tf# Set up the data. x_values = np. random. rand(100, 1)y_values = 2*x_values + 4. 0tf. reset_default_graph()a_weight = tf. get_variable(name= a , shape=[1])b_weight = tf. get_variable(name= b , shape=[1])x_placeholder = tf. placeholder(shape=[1, None], dtype=tf. float32)y_placeholder = tf. placeholder(shape=[1, None], dtype=tf. float32)loss = tf. square(x_placeholder * a_weight + b_weight - y_placeholder)optimizer = tf. train. GradientDescentOptimizer(learning_rate=0. 01)update_op = optimizer. minimize(loss)init = tf. initialize_all_variables()# Launch the tensorflow graphwith tf. Session() as sess: sess. run(init)  for i in range(1000):  rand_indices = np. random. randint(len(x_values), size=10)  rand_x = x_values[rand_index]. ravel()  rand_y = y_values[rand_index]. ravel()  _, a_weight_run, b_weight_run = sess. run(    [update_op, a_weight, b_weight],    feed_dict={x_placeholder:[rand_x],          y_placeholder: [rand_y]})  print a_weight_run, b_weight_runIn the above example, I have set up a dataset corresponding to the points in a straight line y = ax + b. While you can see from the code that the values of a and b are 2 and 4, assume you didn’t have access to the data generation part and you were just given the data. Since we want to determine a and b, we define two variables a_weight and b_weight, by using get_variable() of TensorFlow. We then set up placeholders x_placeholder and y_placeholder that will be used to pump data to TensorFlow. The code pumps 10 randomly selected items of (x, y) each time and TensorFlow tries to determine the weights a_weight and b_weight that minimize the “loss”. Each time, it keeps iterating over its estimate of these weights. If everything goes well, the above code should print values that are approximately 2 and 4. "
    }, {
    "id": 20,
    "url": "http://localhost:4000/procrastination-filter/",
    "title": "Procrastination Filter",
    "body": "2015/10/14 - Have you ever noticed drowning into a sea of articles that you opened via Hacker News? Did you end up buying those nine little things from Amazon of which you don’t use any? If you think you didn’t, then the very fact that you are reading this article means you have been goofing around the Internet. But if you do feel swamped by those 40 tabs on chrome, then I highly urge you to apply this Procrastination Filter to cure your internet inflicted ADD brain. The trick is simple. When you come upon an ‘interesting’ link, tell yourself that you will open it later. Procrastinate it. Chances are that you will forget it, but if it was something important you wouldn’t. So the very act of procrastination acts like a filter separating the gold from the mud. Note that you will not come last in the race if you failed to constantly keep yourself updated (its a rat race anyways). Alright, jokes aside, try this next time when you are planning to buy something online: procrastinate. "
    }, {
    "id": 21,
    "url": "http://localhost:4000/popular-weekend-programming-languages/",
    "title": "Popular Weekend Programming Languages",
    "body": "2013/05/27 - What are some languages used most often during the weekends? Are there some languages that are inherently more ‘hobbyist’ than others? I have attempted to answer these questions for this year’s GitHub Data Challenge. Here is my submission. One way to answer these questions is to survey thousands of programmers about their language use over weekdays and weekends (which might be fairly difficult, and may not be economically viable). But fortunately for us, GitHub records a swath of data from which such information can be mined. Whenever programmers push code to GitHub, or do other activities such as forking, downloading etc. , information is recorded. The data is available to be downloadable as json files at the GitHub Archive, or as a dataset at Google BigQuery. I used the latter. One could argue that of all the type of events performed, PushEvents could indicate the language use better than other events (such as WatchEvents). There are definitely several limitations with this approach but nevertheless, let us stick with this metric. The results when all events are used, is shown in subsequent sections. The high level overview of what I did is as follows: for each language, the number of pushes during the weekends is counted and is divided by the total number of pushes (for that language) to get a ratio for that language. This ratio will indicate roughly how active these programming languages have been during weekends, and so I used these ratios to rank the languages from the most weekend-oriented to the least weekend-oriented. Results: (Here, in order to avoid too many languages, I counted only those languages which had consistently all non-zero number of events every single day. ) So Lua, D and Arduino seem to be some of the most popular languages during the weekend. Then come Common Lisp, Haskell and Clojure (what a coincidence!). Go also seems to be pretty popular during the weekends. I am not too sure what Verilog, and VHDL are doing in the middle! Coffeescript, Actionscript - yes, they make sense. I would have expected Javascript to be around the top (meaning left), but I Javascript is the most popular language (see below) and people are churning out javascript irrespective of the day. Python, Java, PHP are getting relegated to the bottom (right) - these are more for work than hobby. Matlab and R seem to be some of the least frequently used during weekends. Next, let us rank this percentage use during weekends for the most popular languages. To determine language popularity, here are the top-ten languages ranked, determined by counting the number of PushEvents (the number after the language is the raw number of events counted from the dataset).  JavaScript,8538319 Java,5554016 Ruby,5032303 Python,4285711 PHP,4183326 C++,2645128 C,2620992 Shell,1330710 C#,978429 Perl,917207For these languages, the weekend popularity is as follows: Note that these ratios are not that different, so I would argue that one shouldn’t try to find out why C is before C# etc. But the overall point here is that Perl seems to be used more often during weekends than Java. Results based on all Event Types: Why just count the PushEvents? One could as well include the WatchEvents and ForkEvents, and may be something else. To keep things simple, I did the same analysis by counting all types of events. The results are below (again, only those languages are considered which are used everyday).  Most of the pattern followed previously still holds, except now there are a few more languages popping into the diagram. And as before, here are top-ten languages counted by aggregating all events (note the difference between this list and the previous list. Objective-C was not there above, but is there in this list; Perl is missing from the above).  JavaScript,17310652 Ruby,9813411 Java,9461813 Python,8129598 PHP,7678254 C,5008085 C++,4410267 Objective-C,2241348 Shell,2064431 C#,1831029The corresponding ranking is as follows: The complete list (making sure there have been events for at least half of the total number of days). The results are listed as (language, percentage of events during weekends), and ordered by the percentage in descending order.  Nimrod,35. 4411 ooc,34. 0698 Vala,29. 9059 D,28. 4488 Dylan,28. 041 Lua,27. 5552 Parrot,26. 6202 Arduino,26. 5816 Common Lisp,26. 4666 F#,25. 8841 Scheme,24. 95 Verilog,24. 8702 Io,24. 7242 Haskell,24. 596 Delphi,24. 5892 Pure Data,24. 5808 Turing,24. 4526 SuperCollider,24. 2344 Dart,24. 1175 Emacs Lisp,24. 0541 Arc,23. 7705 Ada,23. 5855 Visual Basic,23. 5394 Clojure,23. 4217 Perl,23. 2815 DCPU-16 ASM,22. 822 AppleScript,22. 7723 Shell,22. 7499 Julia,22. 7151 Assembly,22. 6994 VimL,22. 6768 Elixir,22. 6761 Go,22. 5093 C,22. 2897 C#,22. 084 C++,22. 0538 Smalltalk,22. 003 HaXe,21. 9992 OpenEdge ABL,21. 9477 Gosu,21. 8432 Standard ML,21. 4867 CoffeeScript,21. 4702 Racket,21. 1643 Boo,21. 0025 Prolog,20. 8579 ActionScript,20. 5464 PHP,20. 2422 VHDL,20. 0203 JavaScript,19. 9581 Objective-C,19. 9352 Ruby,19. 8123 Python,19. 5424 Java,19. 4998 Logtalk,19. 3459 Objective-J,19. 3309 Scala,18. 9675 Rust,18. 9617 PowerShell,18. 5705 Factor,18. 0105 Coq,17. 8117 Groovy,17. 3457 Erlang,17. 1946 R,17. 1551 Kotlin,17. 0534 OCaml,16. 8474 XQuery,16. 7247 Rebol,16. 4125 Scilab,16. 0884 Matlab,15. 9729 FORTRAN,15. 5981 ASP,15. 5426 Puppet,15. 4394 AutoHotkey,15. 1776 Nemerle,14. 9266 ColdFusion,14. 4573 Eiffel,14. 0177 Apex,11. 9424 Tcl,11. 0578Workflow: Thanks to Google BigQuery, it was a breeze to extract the required information from approximately 60GB of data (of course after many days of tinkering with manually downloading data, figuring out staring at it continuously). But it wasn’t so much of a breeze to download the output so I could further process them. So I have put the csv files in the Data directory. Feel free to use them. I used all the data on BigQuery (which started from 11th March 2012 until 8th May 2013, for a total of 424 days). The following query lists the number of events per day per language. You can add a type='PushEvent' if you want. The downloaded CVS files are in the data folder of the repository. SELECT day, repository_language, COUNT(day) AS count FROM (SELECT repository_language, UTC_USEC_TO_DAY(PARSE_UTC_USEC(created_at))/1000000/3600/24 AS day FROM githubarchive:github. timeline WHERE repository_language IS NOT NULL)GROUP BY day, repository_languageORDER BY day;Conclusion and Reflections: There are indeed several limitations (see below) of this work. The goal here is to not obtain absolute truths, but to try to glean into vast amounts of data and see what it says. Are there some patterns in it, is there something in here that we don’t know? I definitely think this work tells us something about the various programming languages. And if anything, it has helped me view programming languages from a different perspective. I started out with some other plans:    Compute PageRanks of the various repositories based on the repository fork graph to determine their relative importance. I was mainly interested in doing so since I think there must be a difference between the ordered list of popular repositories (in terms of the number of forks) vs the ordered list of important repositories. For example, Spoon-Knife is the second most forked repository, but its importance should be very low. Getting this graph from BigQuery was very hard and so I had to abandon it.     I wanted to see whether news announcements or product launches cause more people to get interested in that language, and therefore more pushes in that language. But from the limited analysis I did, I am afraid I couldn’t find anything like that (may be that tells us something?). For example, Bit. ly announced a realtime distributed message processing system called NSQ on 9th October 2012 and I saw a nice discernible spike in the number of watch events, but I couldn’t see any such trend for the push events. Hopefully I will look more closely into it in the future. Meanwhile, more comments are welcome.  Limitations:  The data is limited to only those programmers who use GitHub (gasp!).  There are definitely many programmers who don’t use GitHub for their work. So all those repositories are not taken into account (which is probably good).  GitHub classification of languages is not always accurate (see http://datahackermd. com/2013/language-use-on-github/#comment-798271901).  I haven’t accounted for different timezones. While it may not be too difficult to account for it, I think its not worth the effort. Only location information of the programmers are recorded, which can be ambiguous/inaccurate. Your comments: How would you interpret these results? What else do you think can be done? Let me know your comments! "
    }, {
    "id": 22,
    "url": "http://localhost:4000/xoring-elephants-novel-erasure-codes-for-big-data/",
    "title": "XORing Elephants: Novel Erasure Codes for Big Data",
    "body": "2013/04/23 - This is a post that has been long overdue. Our paper, “XORing Elephants: Novel Erasure Codes for Big Data” is now accepted for publication at VLDB 2013. More details about the project, and the paper can be found from the project page: Xorbas Hadoop System. [Illustration by Dr. Dimakis] Here is the abstract:  Distributed storage systems for large clusters typically use replication to provide reliability. Recently, erasure codes have been used to reduce the large storage overhead of three-replicated systems. Reed-Solomon codes are the standard design choice and their high repair cost is often considered an unavoidable price to pay for high storage efficiency and high reliability.  This paper shows how to overcome this limitation. We present a novel family of erasure codes that are efficiently repairable and offer higher reliability compared to Reed-Solomon codes. We show analytically that our codes are optimal on a recently identified tradeoff between locality and minimum distance.  We implement our new codes in Hadoop HDFS and compare to a currently deployed HDFS module that uses Reed-Solomon codes. Our modified HDFS implementation shows a reduction of approximately 2x on the repair disk I/O and repair network traffic. The disadvantage of the new coding scheme is that it requires 14% more storage compared to Reed-Solomon codes, an overhead shown to be information theoretically optimal to obtain locality. Because the new codes repair failures faster, this provides higher reliability, which is orders of magnitude higher compared to replication. Please also take a look at my post on dummies guide to erasure codes if you are new to the field. As you can read from the abstract, the main problem when using erasure codes is that they generate a lot of repair traffic (whereas replication uses a lot of storage). So our codes try to find a middle-ground between the repair traffic and the storage tradeoff. One thing I want to emphasize here is that, in data centers where there are thousands of servers, failure is the norm rather than the exception. Hopefully I will get to talk a little bit more about failures and the amount of repair traffic generated in data centers in a different post. Stay tuned! "
    }, {
    "id": 23,
    "url": "http://localhost:4000/eight-queens-puzzle/",
    "title": "Eight Queens Puzzle",
    "body": "2013/03/29 - I came across a very nice puzzle called the Eight Queen Puzzle a few days back. Although, I must agree at the outset that it is somewhat sad that I have not heard this one before. I came to know about this puzzle through this random Quora post that had showed up on my Quora weekly digest. The sentence “25-year-old Prolog skillz to solve 8 Queens” from the post totally did not make any sense to me and so I had to look up what it meant, which led me to come to know about Prolog and this puzzle. I am as time-constrained as you are (I know this is debatable), and so I read what the puzzle was about from the wikipedia post and closed that tab. Surprisingly, I stumbled upon the puzzle again the next day. This time it piqued my interest and here I am, writing this post about it along with the code I implemented. The problem goes as follows (taken from Wikipedia):  The eight queens puzzle is the problem of placing eight chess queens on an 8×8 chessboard so that no two queens attack each other . Beautiful, isn’t it? In fact, the problem can be generalized to any N, i. e. we want to place N queens on an NxN board, of course while not making the queens get mad at each other. I think there are many ways of solving this puzzle. Looks like there is a very nice succinct implementation using Python (refer to the wikipedia entry), but since I sail smooth in the backwaters of Java, I decided to handle this problem via Java. It is fairly straightforward to realize that a brute force solution will be hugely inefficient since there are $64 \choose 8$ possibilities (this might not take long, but this approach does not scale). Next it is very clear that since there are N queens and there are N rows (and columns), each queen must be on a different row (column). So the way to solve this problem would be to determine the correct location of on each row to place queens. You start with the first row, try to place the queen at the first location in the row, then see whether you can place the remaining queens in the remaining rows. But keep in mind that as soon as you keep a queen, that particular column is also ruled out, and so will the diagonal. Therefore the search space keeps decreasing rapidly. You can see that there is a structure to this problem. The template to solve the problem is as follows: for the given row, you try to place the queen on a valid segment, then try to solve the smaller problem of placing the queens in the remaining rows. This leads itself to recursion very nicely. Another advantage of recursion is that backtracking becomes easier. When we hit the last row, it might turn out that none of the positions are valid. Then without any worry, we can revert back to the row before the last, try the next possible position and again test the last row. public class EightQueens { public static void main(String args[]) {  int N = 8;  int[][] board = new int[N][N];  solve(0, board, N);  for(int i = 0; i &lt; N; i++) {   for(int j = 0; j &lt; N; j++) {    if(board[i][j]==1) System. out. print( Q  );    else System. out. print( *  );   }   System. out. println();  } } static boolean solve(int row, int[][] board, int N) {  if(row&gt;=N) return true;  for(int position = 0; position &lt; N; position++) {   if(isValid(board, row, position, N)) {    board[row][position] = 1;    if(!solve(row+1, board, N)) {     board[row][position] = 0;    } else     return true;   }  }  return false; } static boolean isValid(int[][] board, int x, int y, int N) {  int i, j;  for(i = 0; i &lt; x; i++)   if(board[i][y]==1)    return false;  i = x - 1;  j = y - 1;  while((i&gt;=0)&amp;&amp;(j&gt;=0))   if(board[i--][j--]==1) return false;  i = x - 1;  j = y + 1;  while((i&gt;=0)&amp;&amp;(j&lt;N))   if(board[i--][j++]==1) return false;  return true; }}Here is a sample solution for N = 8: Q * * * * * * * * * * * Q * * * * * * * * * * Q * * * * * Q * * * * Q * * * * * * * * * * * Q * * Q * * * * * * * * * Q * * * * There are many limitations, such as the fact that I am displaying only one solution. That can be handled very easily, by tweaking how the recursion starts. One can also use BitSet or other techniques to reduce the storage space required. Using integers to store 0 or 1 on an NxN grid is indeed wasteful, but I think doing all that will obscure the way the solution is presented. Iterative solutions are possible, too. I hope you enjoyed this puzzle. If you find any bugs or a better solution or a way to make the program smaller, please let me know. "
    }, {
    "id": 24,
    "url": "http://localhost:4000/kneser-graphs/",
    "title": "Kneser Graphs and the EKR Theorem",
    "body": "2013/01/27 - In the last post, I briefly talked about vertex cover, edge cover, maximum matching and independent sets. The fractional versions of these problems are not too far away from the integral solutions, but this is not the case with the chromatic number (and its dual clique number). This is in particular true for Kneser graphs, a very interesting class of graphs. The gap between fractional chromatic number and integral (perfect) chromatic number can be arbitrarily large for Kneser graphs. The Kneser graph is defined as follows: For integers n and k with \(n \ge k\), all k-subsets of {1, 2, …, n} are considered as vertices. Two vertices are connected if and only if the corresponding vertices do not have any common elements.  Figure: The Kneser Graph KG(5,2) isomorphic to the Petersen graph (source: Wikipedia) Kneser, who came up with these graphs, conjectured that the chromatic number is \(n − 2k + 2\), when \(n \ge 2k-1\) and this was proved by László Lovász in 1978. In contrast, the fractional chromatic number is \(n/2k\) (when \(n \ge 2k\)), which explains the gap. Note that the local chromatic number, defined by Erdős, lies in between these two values. I found this paragraph in the Wikipedia article on Kneser graphs very interesting:  As Kneser (1955) conjectured, the chromatic number of the Kneser graph KGn,k is exactly n − 2k + 2; for instance, the Petersen graph requires three colors in any proper coloring. László Lovász (1978) proved this using topological methods, giving rise to the field of topological combinatorics. Soon thereafter Imre Bárány (1978) gave a simple proof, using the Borsuk–Ulam theorem and a lemma of David Gale, and Greene (2002) won the Morgan Prize for a further simplified but still topological proof. Matoušek (2004) found a purely combinatorial proof. The rest of the post has nothing to do with chromatic numbers, but I wanted to highlight an interesting theorem called the Erdős-Ko-Rado (EKD) theorem and how it can be used to directly upper bound the independence number of a class of Kneser graphs. Erdős-Ko-Rado theorem is the following: Consider n, k with \(n \ge 2k\). Let A be the k-subsets of {1, 2, …, n} such that each pair of these k-subsets intersect. The maximum cardinality of A is given by \({n-1 \choose k-1}\). Lets recall that independent set consists of all edge disjoint vertices. So in Kneser graphs, any independent set consists of vertices such that any two have at least one element in common. This means that maximum size of the independent set, given by the Erdős-Ko-Rado theorem is \({n-1 \choose k-1}\). But one has to wonder how tight this will be? For KG(5,2), the independent set has size 4; while using the above upper bound gives us \({4 \choose 1} = 4\). "
    }, {
    "id": 25,
    "url": "http://localhost:4000/a-few-notes-on-graph-theory/",
    "title": "A few notes on graph theory",
    "body": "2013/01/24 - I would like to summarize very briefly a few of the most basic problems in graph theory. It is my hope to write more about these and about graphs later, so I can refer to this page when required. I am assuming here that you know what are graphs and you know the basics of graph theory and about linear programs and integer programs. Assume that a graph G=(V,E) is given and has n vertices and m edges. Minimum Vertex Cover: Find the minimum number of vertices required to cover all the edges, i. e. each edge should have at least one of its vertices part of the vertex cover. This can be written as an integer program. Consider variables $x_u \in {0, 1}$ to indicate whether vertex u has been selected into the vertex cover or not ($x_u = 1$ means the vertex is in the cover, else not). The goal is to minimize the sum of all $x_u$. But we want to make sure that if $e = (u, v) \in E$, then at least one of u or v must be in the cover, so $x_u + x_v \ge 1$. Therefore, the program can be written as: [\begin{align}\min &amp; \sum_{u=1}^n x_u s. t. &amp; x_u + x_v \ge 1 \qquad \forall e = (u, v) \in E &amp; x_u \in {0,1}\end{align}] It is known that the vertex cover problem is NP-Hard and so solving this integer program is also NP-Hard. But if we relax the integrality constraint to allow for fractional solutions (i. e. $0 \le x_u \le 1$), it becomes a general linear program which can be solved in polynomial time. [\begin{align}\min &amp; \sum_{u=1}^n x_u s. t. &amp; x_u + x_v \ge 1 \qquad \forall e = (u, v) \in E &amp; x_u \ge 0\end{align}] The solution to this problem is called Fractional vertex cover. Note here that we don’t have to consider $x_u \le 1$ because it is implied. The solution that we get due to such a relaxation on the integer constraint is often useful. In many problems the difference between the value obtained using this relaxation may not be too bad from the optimal value. And sometimes it may be used to round off values to integers so that one can get a good integer solution. Now let us consider a few more problems. Minimum Edge Cover: The minimum number of edges required to cover all the vertices. That is, each vertex should have at least one edge from the cover incident on it. The corresponding integer problem is: [\begin{align}\min &amp; \sum_{e=1}^m y_e s. t. &amp; \sum_{\forall e \ni u} y_e \ge 1 \qquad \forall u \in V &amp; y_e \in {0,1}\end{align}] where $e \ni u$ means that the edge e is incident on u or in other words, u is one of the end vertices of e. Maximum Independent Set: Find the maximum number of vertices that are disjoint in a graph. Consider $x_u$ to be indicator variables as before. [\begin{align}\max &amp; \sum_{u=1}^n x_u s. t. &amp; x_u + x_v \le 1 \qquad \forall e = (u, v) \in E &amp; x_u \in {0,1}\end{align}] Maximum Matching: Find the maximum number of edges in a graph such that no two edges share any vertex. [\begin{align}\max &amp; \sum_{e=1}^m y_e s. t. &amp; \sum_{\forall e \ni u} y_e \le 1 \qquad \forall u \in V &amp; y_e \in {0,1}\end{align}] The linear programs of minimum vertex cover and max independent set, and those of the minimum edge cover and maximum matching seem similar, but they don’t have much bearing with each other. In fact, minimum vertex cover and maximum matching, and minimum edge cover and maximum independent set are duals. On a similar note, it turns out that graph coloring problem and clique problems are duals of each other, though I haven’t talked about these in this post. "
    }, {
    "id": 26,
    "url": "http://localhost:4000/the-two-sum-problem/",
    "title": "The Two-Subset Sum problem",
    "body": "2013/01/22 - One of the classic questions is the two sum problem or the two-subset problem:  Given an unsorted integer array A and an integer s, find all the two-tuples that sum up to s Lets note a few things here. The question states explicitly that the array is unsorted. And it tells us to find ALL the pairs, not just one. It doesn’t say whether the elements are distinct or not. There are two ways to tackle the problem. One is to use a longer runtime to save on the space, and the other is to use a slightly bit more storage to run faster. The first method involves sorting the array before searching for the tuples, and the second method involves using a hash table. Sorting the array: First assume that there are no duplicate elements. Here we will solve the problem by saving on the space and running a bit longer. In particular, if the array is of length n, we will use $O(n)$ space, and the runtime will be $O(n\log n)$. Note that the runtime complexity is mainly due to sorting. If we don’t sort the array initially, a naive algorithm is to check for all pairs, and this will take $O(n^2)$. Can we do better? Yes, by sorting the array first. This will take $O(n\log n)$, but will most importantly will give us the crucial structure that is required for searching the pairs. So sort the array first. We will consider two locations and check whether the sum of the values at those locations add up to s. Define two integers i and j. We will sweep i from the left to the right, and sweep j from right to the left. So we can see the following three properties:  i either stays the same or increases.  j either stays the same or decreases.  i &lt; j. Now let us see what to do when sweeping the array. Consider a particular value of i and j. If A[i]+A[j]&gt;s, then one can be confident that increasing j will only make things worse, since A[j] will increase. But if A[i]+A[j] &lt; s, we should increment i with the hope that A[i] + A[j] will increase. If the sum matches s we should become happy and print the tuple. We can increment i and also decrement j. Keep doing this to print all the tuples. Since i and j have scanned the entire array together while maintaining i &lt; j, it turns out that this scanning should have taken O(n). Thus, the effective runtime complexity is $O(n\log n)$ (thanks to sorting for helping us while also limiting us). Now if there are duplicate elements, what do you do? If we actually want to print all duplicate sums, then incrementing i or decrementing j as before at each step might hurt us. For example is s = 8, and if the array is [2, 2, 4, 5, 6, 6], then the above algorithm will not print all the four tuples. Let A[i] + A[j] = s. Earlier we jumped to i+1 and j-1 and continued checking. Now note down j0 = j; For all j before j0 (while maintaining j &gt; i), if A[j] is same as A[j0], print A[i] and A[j]. Once you are done, increment i and come back to j. This way, in the above example, with the first 2, both 6 will be printed, and then we can go back to the rightmost 6 and jump to the second 2 and sweep through both the 6s again. Unfortunately, the worst case complexity will be $O(n^2)$. Hash based implementation: Hash based implementations can be neat, but they cannot handle duplicates. All you have to do is to put the values in a hash and if s minus that value is also there, then print that tuple. The code is given below. import java. util. Arrays;import java. util. *;class TwoSumProblem {  public static void main(String args[]) {    int[] A = {4,0,1,3,2,6,9,21,10,11,5,7};    int s = 8;    twoSumProblemSort(A, s);    System. out. println( -- );    twoSumProblemHash(A, s);  }  static void twoSumProblemSort(int[] A, int s) {  	Arrays. sort(A);  	int i = 0;  	int j = A. length - 1;  	while(i&lt;j) {  		if(A[i]+A[j]==s) {  			System. out. println(A[i]+ ( +i+ ), +A[j]+ ( +j+ ) );  			int j0 = j--;  			while((A[j0]==A[j])&amp;&amp;(j&gt;i)) {  				System. out. println(A[i]+ ( +i+ ), +A[j]+ ( +j+ ) );  				j--;  			}  			j = j0;  			i++;  		}  		else if(A[i] + A[j]&gt;s) {  			j--;  		}  		else {  			i++;  		}  	}  }  static void twoSumProblemHash(int[] A, int s) {  	HashSet&lt;Integer&gt; h = new HashSet&lt;Integer&gt;();  	int i = 0;  	while(i &lt; A. length) {  		if(h. contains(s-A[i])) {  			System. out. println(A[i]+ , +(s-A[i]));  		}  		h. add(A[i++]);  	}  }}Let me know your thoughts, alternative ways of solving this problem or if you find any bugs! "
    }, {
    "id": 27,
    "url": "http://localhost:4000/no-more-rush-hours-an-introduction-to-vehicular-networks/",
    "title": "No more rush hours: An introduction to vehicular networks",
    "body": "2012/11/29 - Imagine doing a 70 on a highway on your convertible and imagine the loud music blaring through the wind. Imagine a sunny saturday when you are making your way to the beach, when suddenly your car tells you to slow down. There has been an accident ahead of you and the traffic is going to get to a crawl. As it turns out, some other car in front of you told your car that the traffic is going to slow down and that car came to know from another car ahead it and so on. Imagine cars talking to one another. A vehicular network is an ad-hoc wireless networking technology that can be used to form a mobile network between cars and other vehicles, mobile or stationary; and between the vehicles and roadside access points. When I say ad-hoc wireless network, I mean the kind of network that is formed on the spur and the constituent nodes can route information between one another. One can call Vehicular Networks as VANET, and the vehicle-to-vehicle communication as V2V. Vehicle to infrastructure communication is called V2I. Add some brains to a vehicle, it can do a lot of stuff. Add networking and it opens up an amazing range of applications. The brains here refers to having a computer or a processor that can run a few set of programs. Consider the following example. Attach an accelerometer and a GPS to a car. When the vehicle lands into a pothole, the accelerometer fires and this can be recorded along with the GPS coordinates. Its now not hard to see that multiple cars will have recorded a spike in the accelerometer readings at about the same GPS coordinates. So if I have access to all these data, I can infer sitting at my desk that there is a pothole at a particular GPS coordinate.  This is precisely what “MIT Pothole Patrol project” accomplishes. You may ask why do we need vehicular networks? Let me ask that to you - do we really want to invest time and money in getting cars to talk to one another? Do we really care if a car can figure out that there is another can coming very fast towards the intersection even though it is red? Do we want to have a technology that can potentially save thousands of lives per year? If you are like me, I assume your answer to these questions is an yes. When do you think they may be available, you may ask. I don’t know. May be in five years, or in ten years. The automotive industry is slow to respond to innovation, partly because the lives of millions are at stake. And partly because the culture is different. There are many safety applications possible. For example, detecting fast approaching vehicles at intersections, broadcasting traffic information to other vehicles behind and so on. Furthermore, there can be many other non-safety based application possible - such as detecting potholes and sending back the information to a central server. Another rising star in the vehicular domain is the Autonomous vehicle - a self-driving car. Such cars do hold a lot of promise - not only in decreasing the accidents, but also in decreasing the congestion in the roads. For one, a family may not need multiple cars, since for example, the car can leave a person to his office and come back to help the spouse. This means less vehicles on the roads, and consequently less congestion. Cars can also communicate with each other to figure out when to accelerate and when to decelerate, and to keep close distance to minimize congestion (for example when the signal turns green the cars can start accelerating faster than if a human were driving so the congestion clears out faster). Such applications not just require autonomous driving capabilities - but also the vehicular network. You can think of all sorts of crazy applications now - swarms of cars going together to minimize the wind resistance, thereby getting better fuel economy and so on. Autonomous cars, if they become a reality will open up a slew of new applications, and if these cars can talk with one another, will change how humans move around. But for now, I must drink coffee so I don’t fall asleep behind the wheel. "
    }, {
    "id": 28,
    "url": "http://localhost:4000/surfing-analogy-for-startups/",
    "title": "Surfing analogy for startups",
    "body": "2012/10/13 - There was an interesting talk a few days back in USC by Michael Sheha (his bio at the end of the post). One of the analogies he gave was how startups were like surfing waves. The market is the wave and one needs to choose a big wave, a really big one. Otherwise there is no fun and no revenue. And the wave is something you can’t control. Its simply there and will come gushing. You cannot try to control it or tell it to movein another direction. You will have to ride along and ride skillfully.  Another analogy he gave was that a big company is like a warship or a tanker, theycannot change direction immediately, whereas you as a startup are like a PT boat -you are nimble, agile, fast and can change direction immediately (if you are not thenyou are doomed, pretty much). One of the proverbs that really caught my attention:  The road to someday leads to the town of nowhere. Here are other things that he talked about:  Customers really provide feedback when they PAY for something.  Keep costs low, really really low. You will never know when you will need the money. Youshould not repent that you could have saved earlier now that your funding got delayed.  Message matters - refine it a thousand times. Here is his bio:  Michael Sheha is currently in-between startups. Previously he co-founded Networks In Motion (NIM) in 2000, which provided Location Based Services (LBS) to the world’s wireless carriers as a private labeled application and cloud-based solution, such as wireless personal navigation for mobile devices. NIM was started in 2000 and grew to over 300 employees internationally and to over $75m/yr in revenue. In 2009 NIM received the Southern California LAVA Award for the Best Exit in Internet &amp; Technology when it was acquired by a public company. Prior to co-founding NIM, he worked at the California Institute of Technology Jet Propulsion Laboratory’s wireless communication systems and research section responsible for the design and development of digital and radio frequency communication systems, military GPS tracking systems, and the R&amp;D in communication link and propagation studies. Michael graduated from University of Southern California (USC) with a MSEE in 2000, and Rensselaer Polytechnic Institute (RPI) with a BSEE in 1995. He is currently living in Southern California, CA with his wife and four children. "
    }, {
    "id": 29,
    "url": "http://localhost:4000/liveblogging-osdi-2012-tuesday/",
    "title": "Liveblogging OSDI2012 - Tuesday",
    "body": "2012/10/09 - There are four main sessions today. Looking forward to the Google talk on Spanner. Day 1 was great (including the food, of course). Looking forward to a great second day. I will blog on the talks as much as I can. Distributed Systems and Networking: DJoin: Differentially Private Join Queries over Distributed Databases: Lots of data accumulated everywhere - social networks, hospitals, airlines etc. So how do we make sense out of this distributed set of data. For example, if you are a medical researcher trying to determine outbreak of a disease based on the hospital data and the airline data (by performing a join), then not only is it dangerous to privacy, but may be it will be illegal.  Idea 1: give all data to a trusted party - but this may not exist.  Idea 2: use secure multiparty computation but may talk long Idea 3: Use Differential Privacy. The paper DJoin is about how to enable answering of queries about private data that is spread across multiple different databases. DJoin supports many SQL like queries - which is great for the users. Evidently, the paper uses idea 3. Security: Improving Integer Security for Systems with KINT: Because the integers in C don’t have unlimited precision, it is possible for them overflow. For example 2^30 * 2^2 = 0. This can be exploited by attackers. One of the famous examples is that of the iPhone jailbreak. Another is the example of logical bugs in linux kernel. There is an OOM killer which assigns scores to processes based on memory usage and then kills the processes with the highest score. This can be exploited by malicious code that can take a lot of memory but still not be detected (because by overflow their scores can get evaluated to 0). It is in fact hard to prevent integer overflows, even if you have unlimited precision (there could be other bugs or it is difficult). Contributions of KINT:(1) a case study of 114 bugs in the linux kernel. (2) KINT: a static analysis tool for C programs used to find the 114 bugs. Case study: Linux kernel. The 114 bugs have been confirmed and fixed by developers. Most are memory and logical bugs. Writing correct checks is non-trivial. KINT has the following modules:(1) Bound check insertion. (2) Taint analysis. (3) Range analysis. KINT Advocates the use of NaN (instead of 0 when overflow occurs). Details at http://pdos. csail. mit. edu/kint/ Dissent in Numbers: Making Strong Anonymity Scale: Talk about how to scale anonymous communication to large number of users.  Tor: The onion router. Tor is not timing analysis resistant.  DC-netAbstract:  Current anonymous communication systems make a trade-off between weak anonymity among many nodes, via onion routing, and strong anonymity among few nodes, via DC-nets. We develop novel techniques in Dissent, a practical group anonymity system, to increase by over two orders of magnitude the scalability of strong, traffic analysis resistant approaches. Dissent derives its scalability from a client/server architecture, in which many unreliable clients depend on a smaller and more robust, but administratively decentralized, set of servers. Clients trust only that at least one server in the set is honest, but need not know or choose which server to trust. Unlike the quadratic costs of prior peer-to-peer DC-nets schemes, Dissent’s client/server design makes communication and processing costs linear in the number of clients, and hence in anonymity set size. Further, Dissent’s servers can unilaterally ensure progress, even if clients respond slowly or disconnect at arbitrary times, ensuring robustness against client churn, tail latencies, and DoS attacks. On DeterLab, Dissent scales to 5,000 online participants with latencies as low as 600 milliseconds for 600-client groups. An anonymous Web browsing application also shows that Dissent’s performance suffices for interactive communication within smaller local-area groups. Efficient Patch-based Auditing for Web Application Vulnerabilities: Example:GitHub vulnerability publicly announced in March 2012. Once this is known, it is of interest to find out who exploited this vulnerability (detecting past attacks). Github didn’t audit themselves because detecting past attacks is hard. And there are too many vulnerabilities to check manually. Approach here is to automate auditing using patches. Auditing many request is tough. Auditing one month request may take two months. In their system, auditing can be 12-51x faster than original execution. Potpourri: Experiences from a Decade of TinyOS Development: Philip Levis, Stanford University 1999: Wireless Sensor networks coming up. TinyOS was born. Thirteen years later: 25k downloadsand a worldwide community of developers. Outline:  Two design principles for embedded software a technical result: static virtualization a lesson: avoid the island syndromeMinimize resource use: RAM, code space, energy. Sleep current necessitates micro-controllers. Advanced apps run into ROM/RAM limits. You really need to structure your code to avoid bugs because debugging is brutally difficult. Once the device is deployed in the wild, its impossible to debug. Static virtualization:  Allocates exact RAM no pointers cross-call optimization dead code elimination compile-time certainty. Everything is allocated statically. Callback functions are specified at compile time rather than run time. Two nines of reliability obtained after doing static virtualization. Apps became highly robust. Island Syndrome:TinyOS technically focused on enabling users to build larger, more complex applications. Doing so increased the learning curve to building simple ones! Code evolved to use nesC features in more complex and intricate ways. Increased barrier to entry: island syndrome. Phil Lewis expressed dissatisfaction as to how this has become an island, and that people are instead using other products like Arduino which are very user-friendly. Arduino doesn’t even have networking (not sure). Contiki (for the internet of things) seems to be another thing that people are embracing. TinyOS has been academically successful, but people are not using it as much as it has the potential. Overall, it was a very good talk. Automated Concurrency-Bug Fixing: Guoliang Jin, Wei Zhang, Dongdong Deng, Ben Liblit, and Shan Lu, University of Wisconsin—Madison Buggy software is an unfortunate fact. It will be great to automate bug fixing, but thats difficult. In multi-core era, we may see more and more concurrency bugs. Abstract:  Concurrency bugs are widespread in multithreaded programs. Fixing them is time-consuming and error-prone. We present CFix, a system that automates the repair of concurrency bugs. CFix works with a wide variety of concurrency-bug detectors. For each failure-inducing interleaving reported by a bug detector, CFix first determines a combination of mutual-exclusion and order relationships that, once enforced, can prevent the buggy interleaving. CFix then uses static analysis and testing to determine where to insert what synchronization operations to force the desired mutual-exclusion and order relationships, with a best effort to avoid deadlocks and excessive performance losses. CFix also simplifies its own patches by merging fixes for related bugs. Evaluation using four different types of bug detectors and thirteen real-world concurrency-bug cases shows that CFix can successfully patch these cases without causing deadlocks or excessive performance degradation. Patches automatically generated by CFix are of similar quality to those manually written by developers. Spanner: Google’s Globally-Distributed Database: Good start with the host replicating his welcome message twice! Google is sharding the data to 1000s of machines, across multiple countries in different continents. Here is an overview of the talk:  lock-free distributed read transactions.  theoretical property: external consistency of distributed transactions (first system at global scale).  implementation: integration of concurrency control, replication and 2PC enabling technology: TrueTime. Exposes uncertainty in the clock by exposing as intervals. Why does latency matter? Bing engineers artificially injected delay and saw revenue going down. Geo-replication is used by major providers of internet services e. g. Google, Amazon, Facebook. Some solutions adopt strong consistency, whereas others eventual consistency. Paxos vs Dynamo, Bayou. High latency vs low latency but undesirable behaviors. The author is saying that the can guarantee both strong and eventual consistency. They have built a system called gemini. I definitely need to take a look at the paper in depth because I couldn’t get all the details I wanted to learn from the talk. Overall, OSDI conference has been great and got to hear some excellent talks. More importantly, met lots of people from all over the world! "
    }, {
    "id": 30,
    "url": "http://localhost:4000/liveblogging-osdi2012/",
    "title": "Liveblogging OSDI2012 - Monday",
    "body": "2012/10/08 - I am attending OSDI 2012 here at Hollywood, CA. Lots of interesting papers here and I will try to blog about this event. In particular I am excited about attending Google’s spanner talk scheduled for tomorrow afternoon (Tuesday). The day didn’t begin too well, because I happened to witness a roadside accident. I was on the bus going to the Loews hotel (where the conference is going on), and the bus was waiting on red. It turned green and even before the bus moved ahead, a white Toyota Prius sped to turn left. Another car came dashing on the right of the bus lane because clearly it was green for it and before anybody noticed, there was a boom and a woman shouting - the Prius was hit on its right passenger side door. From what I figured out there was no injury of anybody. Other people were busy and my bus moved on. While this was a stupid accident that could have been avoided, I wish Vehicular Networks were mainstream now. If the Prius had alerted the driver about a car coming towards it, hopefully it wouldn’t have turned left prematurely. While clearly, vehicular judgements is not a replacement for poor judgement, stuff happens sometimes and I strongly believe vehicular networks can help in some cases (for example if today the other car was jumping red while say the Toyota was turning left on a green arrow). Today the talks have been on Big Data, Privacy and Mobility. I am posting as much as I can. . Keynote: The keynote is on cancer genomics. The speaker is David Haussler from UCSC. Here is the abstract:  Cancer is a complex condition—patients present with thousands of subtypes involving different combinations of DNA mutations. Understanding cancer will require aggregating DNA data from many thousands of cancer genomes, facilitating the statistical power to distinguish patterns in the mutations. The rapidly plummeting cost of DNA sequencing will soon make cancer genome sequencing a widespread clinical practice. To anticipate this, UCSC has built a 5-petabyte database for tumor genomes that will be sequenced through National Cancer Institute projects—the Cancer Genomics Hub—and is tackling the significant computational challenges posed by storing, serving, and interpreting cancer genomics data. Some of the questions/points raised:  There is an enormous opportunity to bring big data techniques to cancer genomics.  How do we find out mutations from these gene data? How to map these mutations to the pathways that lead to cancer, which could possibly help us prevent these cancers. So looks like UCSC has built a 5 PB data center just for this purpose. Flat Datacenter Storage: FDS is a general purpose blob store. Here are some of the prominent points to note:  FDS is simple, scalable blob storage distributed metadata management Built on a CLOS network with distributed scheduling.  High read/write performance fast failure recovery High application performance. Data is organized as blobs, and each blob has multiple tracts. Consists of Tractserver (sits between raw disk and network), Metadataserverand a client. GFS/Hadoop have the following problems:  Centralized metadata server critical path of reads/writes large (coarsely striped) writesBut DHTs require multiple hops to find data and have slow recovery. FDS tries to position itself in between. There is a tract location table, that maps for each locator the disks it has to read. CLOS:: Generally we have this tree structure for the DC architecture. FDS provisions as much bandwidth as each disk requires. Full bisection bandwidth is only stochastic. Long flows are bad for load balancing. FDS generates a large number of short flows are going to diverse destinations But TCP likes long flows. FDS creates “circuits” using RTS/CTS. More disks means faster recovery. This is somewhat in contrast with RAID that might take longer to recover when there are more disks. In a cluster with 1000 disks, when one server was failed that had 7 disks with 655GB, the failure took only about 34 seconds (thanks to full bisection bandwidth), which is quite impressive. Sorting: Minute Sort was won in 2012 by FDS. Jim Gray’s benchmark:  How much data can you sort in 60 seconds? In 2009 Yahoo won the benchmark, and last year a team from UCSD. FDS seems to have won hands down compared to the Yahoo results (I think 1400+ TB vs 500 TB). I think UCSD could sort something like 1300+TB, but their main contribution is in disk IO. One of the things I am concerned is that to provide for such fast read/writes and repair, they have just increased the network bandwidth significantly. Not sure what would be the effect of such improvement in other systems. PowerGraph: Graphs are ubiquitous and are essential for data-mining. Many social network graphs are natural graphs which are derived from natural phenomenon. Existing systems perform poorly on natural graphs mainly because of high-degree vertices and low-quality edge-cuts. The defining characteristic is the power-law degree distribution, which leads to a “star-like” motif. For example Obama is in the center of Twitter. Things like graph partitioning are difficult when there are high-degree vertices. The main idea of PowerGraph is  Split high-degree vertices. These vertices are now replicated to many vertices with one master and multiple mirrors. Algorithms can run parallel on these vertices. There are two existing graph abstractions: Pregel and GraphLab. Both of them resort to random placement of vertices - vertices are selected randomly to place onto nodes. PowerGraph uses GAS Decomposition: Gather (reduce), Apply, Scatter. Albert et al. proved that natural graphs may have good vertex cuts. There is a theorem in the paper that compared to edge-cut, vertex cuts can improve the communication costs. GraphChi: This paper talks on doing large-scale graph computation in a single PC!Big Graphs are not exactly big data, for example Facebook edge data can be fit in one hard drive (140 billion connections is about 1TB). No problem at all! So the main punchline here is that most algorithms must be able to be run on a single disk. Looks like a very promising tool especially when you don’t have access to multiple machines. I could probably revisit the Twitter graph analysis using this tool. Running algorithms used to be very difficult, even on small subgraphs of Twitter. This talk is very similar to the previous one (well mostly in terms of the evaluations). Note: There doesn’t seem to be a good network connection and there are only a few power outlets and my 9 month old macbook pro seems to be draining battery rapidly, so I think live blogging will be replaced by live notetaking. Privacy: I strongly believe that the Hails talk speaker has copied my keynote template colors!! :)He seems to use the same template and the same set of colors - a different shade of red for title and a different shade of blue to highlight things in the text. I had converged on these colors and template after several revisions, so its somewhat spooky that a different user uses almost the same template. I don’t remember posting my talk online, so not sure how I lost my privacy!The main theme of the talk is as follows: Major webplatforms like Facebook depend on third party appsto deliver a rich experience for mobile phone users. But users running third party apps don’t have much control over what these apps do. Hails is a new web framework that gives a principledapproach to code confinement so that untrusted codecan be run safely. The second talk introduced a system called Lacuna. The basic idea is that the current privacy providing features such as using incognito windows are not really privacy providing. Consider an example of somebody browsing a webpage through incognito window. The audio buffer might have some left over data, and similarly the X-Server might have some traces left. The network buffer may also have some data. Lacuna provides forensic deniability. The third talk in this session is about CleanOS. CleanOS is a new Android based OS design that minimizes amount of sensitive data exposed on device at any time. Clean OS brings a new view on data security: minimize and audit exposure of sensitive data to attack. May be applicable to other domains such as Data Centers and web data security. Mobility: Two papers here. COMET: Code offload by Migrating Execution transparently. : What is offloading? Because mobile devices have limited resources, its sometimes a good idea to offload processing to the cloud. The main question is whether we can bring network resources to mobile? Few other past works: MAUI, CloneCloud. These works follow the ‘capture and migrate’ paradigm. There are a few areas of improvement:  thread and sync support offloading parts of methods is difficult. Goals::  improve mobile computation speed     no programmaer effort   generalize well with existing applications   resist network failures: in mobile networks, connectivity is intermittent.    Distributed Shared Memory:: COMET is offloading + DSM. DSM is generally applied to cluster environments with low latency and high throughput. In Munin paper, which was one of the first to do DSM, even writing to a variable might take longer than RTT due to checking with other servers. We want to understand Java memory model. To facilitate migrating, heaps, stacks and locking states are kept consistent across the device and the VM. COMET is implemented by extending the Dalvik virtual machine targeted for Android. The system exhibited a speed up of about 2. 88X on average on 9 applications. AppInsight: Mobile App Performance Monitoring in the Wild: In today’s mobile market, there are at least 1 million apps,more than than 300k developers and an average user uses about 100 apps. In fact, there is an app for everything!But most of the apps are slow. People complain a lot. The speaker showed an example of a restroom finding app with negative reviews like “its too slow when you need it fast”. The point here is that there are too many things that could go wrong in all these mobile devices and it is very difficult for the programmers to know. For example, there can be a diverse set of network connectivity, different mobile handsets etc. A few questions arise in the mind of the developer. What is the user-perceived delay? Where is the bootleneck? Unforunately, there is only little platform support for monitoring. This paper presents AppInsight, automatic app instrumentation. What is great is that it requires zero developer effort. There is no need to check source code, and binary is good. Challenges with app instrumentation:  instrumentation impacts app performance limited app resources highly asynchronous programming pattern (tracing async code is hard)System details:  User Transaction.  Critical path. If you optimize it, it reduces the overall user perceived delay. Analysis: Find the critical paths in user transactions. Also can find the exception path. Deployment:  30 windows phone apps 30 users over 4 months of data collected. Real world results  15% of the user transactions take more than 5 seconds.  apps are highly asynchronous. I think this was one of the best talks today, in terms of the presentation. "
    }, {
    "id": 31,
    "url": "http://localhost:4000/latex-tips-tricks-embed-fonts-vector-graphics/",
    "title": "LaTeX Tips and Tricks",
    "body": "2012/08/17 - There seem to be quite a few techniques to embed the fonts while generating PDFs from LaTeX sources, and I just wanted to collate a few in one post. Further, I wanted to share my complicated workflow in creating vector graphics. Let me know if you have better solutions (in the past I have use inkscape with ‘limited’ success: some dots simply disappeared in Mac’s preview). Embedding Fonts: If you are using pdflatex:: If you have all your figures in PDF and you prefer using pdflatex, then you may want to follow this link. Update:Most of the time I just use pdf images and so I end up using pdflatex. Pdflatex is not only convenient, I think its recommended. I recently stumbled upon this eternal problem of running into non embedded fonts. This time I could quickly point my fingers towards a few pdf plots generated using Matlab. Thanks to Kimo, I was able to fix the problem quickly. All I had to do was to run the following ps2pdf14 -dPDFSETTINGS=/prepress fig_non_embedded. pdf fig_embedded. pdfI believe ps2pdf14 is equivalent to ps2pdf -dCompatibilityLevel=1. 4 (just a guess), and this should also help prevent the error “Acrobat version is less than 5. 0” that you might get on IEEE PDF eXpress. [Note that earlier I had ps2pdf13 used, and I had this issue]. I made a handy script to fix all the figures. #!/bin/shps2pdf13 -dPDFSETTINGS=/prepress  $1   $1. emb rm  $1 mv  $1. emb   $1 pdffonts  $1 If all your images now have embedded fonts, then you should be able to generate an impeccable pdf with all fonts embedded, unless the universe decides against you (which happens quite often with me). If you are prefer the long way:: If you have eps figs or may be pstricks based code for image, then you may want to do latex -&gt; dvi -&gt; ps -&gt; pdf. In that case, I do this: latex filename. texdvips filename. dvips2pdf -dEmbedAllFonts=true -dSubsetFonts=true -dEPSCrop=true -dPDFSETTINGS=/prepress filename. psYou will end up with filename. pdf which should have all the fonts embedded. If you have an EPS image which you want to convert to PDF while making sure the fonts are embedded, you can try epstopdf --outfile='filename-temp. pdf' filename. epsgs -dSAFER -dNOPLATFONTS -dNOPAUSE -dBATCH -sDEVICE=pdfwrite -sPAPERSIZE=letter -dCompatibilityLevel=1. 4 -dPDFSETTINGS=/printer -dCompatibilityLevel=1. 4 -dMaxSubsetPct=100 -dSubsetFonts=true -dEmbedAllFonts=true -sOutputFile=filename. pdf -f filename-temp. pdfrm filename-temp. pdfCredit: Joey Yet another way would be to embed the EPS figure in an article tex file and then compile the above way to generate a PDF. Now you will be stuck with a letter size PDF image, which you can crop (see below). Generating Vector Graphics: I generally use this awesome program called LatexDraw which is great except that it gave me a little bit of a headache for about 4 hours because the PDFs I was creating did not have all fonts embedded. Its not always like that, so I do recommend it highly. I tried xfig and it looked it got teleported from 1995. Install it, and start drawing the block diagram you need. Its quite straight forward. Its cool to see the code getting updated as you draw. In fact you if you can type formulae between $ and $. They will get compiled to the correct formulae, but you may have to try several times to get the position right. Once you are happy and smiling, copy the generated code (they even have a button to do that!), and get ready for some hard work. Again, you have two options. I like covering these images to PDFs and then embed it in my paper and run pdflatex. Instead, if you are going the long way of running latex-&gt;dvi-&gt;ps-&gt;pdf, then you all you may need to do is to copy paste the code. If you want it as a figure and want to add captions, just surround by figure: \begin{figure}[h]\begin{center}% your generated code. \end{center}\caption{The case when $\beta &gt; 1$ or whatever caption you like}\label{beta1}\end{figure}There may be a number of reasons you may not want to do this. May be your mac is complaining that pstricks is not found, in which case I am not sure what to do. I have access to a linux machine which doesn’t complain. But since my pdflatex (in both mac/linux), I use the linux machine to generate the PDF the long way (also an EPS will be nice to have right?). So fire up your latex editor and create a fresh file, like below. \documentclass{minimal}\usepackage[usenames,dvipsnames]{pstricks}\usepackage{epsfig}\pagestyle{empty}\begin{document}\scalebox{0. 9} % Change this value to rescale the drawing. {\begin{pspicture}(0,-0. 53)(5. 92,0. 49)\definecolor{color100b}{rgb}{0. 7058823529411765,0. 7058823529411765,0. 7058823529411765}\psframe[linewidth=0. 036,dimen=outer,fillstyle=solid,fillcolor=color100b](0. 54,0. 47)(0. 08,0. 01)\psframe[linewidth=0. 036,dimen=outer,fillstyle=solid,fillcolor=color100b](1. 2,0. 47)(0. 74,0. 01)\psframe[linewidth=0. 036,dimen=outer,fillstyle=solid,fillcolor=color100b](3. 56,0. 49)(3. 1,0. 03)\psframe[linewidth=0. 036,dimen=outer](5. 92,0. 47)(5. 46,0. 01)\pscircle[linewidth=0. 04cm](1. 62,0. 25){0. 08}\pscircle[linewidth=0. 04cm](2. 2,0. 25){0. 08}\pscircle[linewidth=0. 04cm](2. 7,0. 25){0. 08}\pscircle[linewidth=0. 04cm](3. 94,0. 25){0. 08}\pscircle[linewidth=0. 04cm](4. 34,0. 25){0. 08}\pscircle[linewidth=0. 04cm](4. 74,0. 25){0. 08}\pscircle[linewidth=0. 04cm](5. 12,0. 25){0. 08}\psline[linewidth=0. 04cm](0. 0,-0. 13)(3. 66,-0. 13)\psline[linewidth=0. 04cm](3. 64,0. 07)(3. 64,-0. 11)\psline[linewidth=0. 04cm](0. 0,-0. 13)(0. 0,0. 07)\usefont{T1}{ptm}{m}{n}\rput(0. 258125,-0. 36){$1$}\usefont{T1}{ptm}{m}{n}\rput(0. 918125,-0. 36){$2$}\usefont{T1}{ptm}{m}{n}\rput(3. 258125,-0. 38){$n$}\usefont{T1}{ptm}{m}{n}\rput(5. 678125,-0. 36){$N$}\end{pspicture} }\end{document}If for any reason minimal does not work, try article class. Do the following again: latex figname. texdvips figname. dvips2pdf -dEmbedAllFonts=true -dSubsetFonts=true -dEPSCrop=true -dPDFSETTINGS=/prepress figname. psKeep making sure that the generated PDFs have all the fonts embedded. NOTE: IF YOU USE DOTS IN LATEXDRAW, IT WILL CREATE CORRESPONDING PSDOTS COMMANDS. THIS SEEMS TO CREATE TYPE-3 FONTS WHICH DON’T GET EMBEDDED. MY SUGGESTION IS TO NOT USE DOTS, BUT JUST USE CIRCLES. Figuring this out took like 4 hours only. The output will look like this: but of course in a pdf file. Again make sure it has all fonts embedded. Next course of action is to crop this pdf. There is a handy tool called pdfcrop by Heiko Oberdiek, which might be already installed in your system, otherwise get it from here I generally create a folder called helper and put the pdfcrop. pl file and also the following file #!/bin/sh# usage:# generatePDF &lt;filename without ext&gt;latex  $1. tex dvips  $1. dvi ps2pdf -dEmbedAllFonts=true -dSubsetFonts=true -dEPSCrop=true -dPDFSETTINGS=/prepress  $1. ps pdfcrop  $1. pdf You can just run . /generatePDF filenameMake sure not to give the extension (. tex). Now you will end up with a PDF that you can just embed as usual. Also, if you want to get EPS file, you can do the following: pdftops -eps filename. pdf If you directly want EPS files, save and use the following script #!/bin/sh# usage:# generatePDF &lt;filename without ext&gt;latex  $1. tex dvips  $1. dvi ps2pdf -dEmbedAllFonts=true -dSubsetFonts=true -dEPSCrop=true -dPDFSETTINGS=/prepress  $1. ps pdfcrop  $1. pdf pdftops -eps  $1-crop. pdf rm  $1-crop. pdf You should be all set to go. Force letter dimensions. : I was writing a one page document in latex and found out that it wasn’t really generating 8. 5in by 11in document. So I ended up using: \documentclass[letterpaper, 10pt]{article}\special{papersize=8. 5in,11in}\setlength{\pdfpageheight}{\paperheight}\setlength{\pdfpagewidth}{\paperwidth}% \setlength{\textwidth}{7. 0in}% \setlength{\textheight}{9. 0in}%\setlength{\oddsidemargin}{-0. 3in}% \setlength{\parskip}{0. 0in}% \topmargin -0. 5in \setlength{\parindent}{0in} \setlength{\parskip}{0. 06in}TEXT\end{document}You can play with the numbers and see what uncommenting other parts does. Thats all folks. Happy latexing! "
    }, {
    "id": 32,
    "url": "http://localhost:4000/difference-between-that-and-which/",
    "title": "Difference between that and which",
    "body": "2012/08/15 - I did not really know the difference, until I started reading the book On Writing Well. The author William Zinsser writes:  William Zinsser, On Writing Well - 6th edition %}If your sentence needs a comma to achieve its precise meaning, it probably needs “which”. “Which” serves a particular identifying function, different from “that”. (A) “Take the shoes that are in the closet. ” This means: take the shoes that are in the closet, not the ones under the bed. (B) “Take the shoes, which are in the closet. ” Only one pair of shoes is under discussion; the “which” usage tells you where they are. Note that the comma is necessary in B, but not in A. He also adds that in most situations, “that” might be the correct choice. I never bothered to learn the difference before, did you? "
    }, {
    "id": 33,
    "url": "http://localhost:4000/life-as-an-mdp/",
    "title": "Life as an MDP",
    "body": "2012/07/31 - On one particular day when I was working towards a deadline, I ended up doing two things effectively - bunch of simulations on Markov Decision Processes and a debate with my friend about life. And when I was having dinner, it dawned upon me that navigating life is very much like solving a Markov Decision Process (MDP). First a disclaimer: I am not going to teach you MDP, nor am I going to model your life as an MDP and solve it. And I am not attempting to teach you how to live your life, like my friend did today. States of life: First off, what is an MDP? I have been wanting to write a ‘dummies guide to Markov decision processes’ (like this one), but haven’t been able to sit down to do so. I will do it, but meanwhile, take a look at the wiki page if you are not familiar. Or better, I will explain it in layman terms (which will be imprecise). The general idea is that there are what are called states of an MDP and at each state you are presented with a set of possible actions. So you want to choose the best action that is good for you in the long run. For example, you may be in a restaurant (state) and you may have the option of going for the pizza or the salad. One option may look pretty good in the short term but not in the long term, and another option may actually be good in the long run even though not so attractive in the short term. Which one would you choose? Utility: In the MDP I was working on, there were a few states that were ‘good’ and I had to backtrack and figure out what action to take so that I will most likely reach one of those states (this statement is imprecise but you get the idea what I am trying to say). But the problem was that it was not very clear how to characterize a state as good or bad and there were a few ‘meh’ states. So we started assigning numbers to states that will measure how good a state was. And this assignment was done by something called an utility function. We had come up with a utility function and thought it did a fairly good job, but after a few simulations, I started seeing strange behavior. Some of the actions that I thought should be taken, were not being taken. So coming back to the topic of life, as I said before, I had a discussion with my good friend about it, or about the lack of it in my case (according to him). He insisted that he was living life well and that I must learn from him. I am always at a struggle with life - to grow, to learn new things and do this and that. I always tend to swim against the current. He is always at ease, and goes along the flow and swims with the current. To me, his life looked mundane and to him my life looked awful. Your mileage may vary: Life is very much like an MDP problem. You have a deadline to live and you start out at zero state. Along the way going from the zero to the last state, at whatever state you are in, you want to choose actions that are best suited for you. What I noticed in the MDP was that the entire course of the path from zero to the last state depended on what utility I assign to the seemingly good states. A slight change and the entire course changed. An action that was optimal before was no more optimal. I just realized that my utility was different from his and that made all the difference. So as long as you choose an utility that will make you happy at the end of it all, your way of life is the right way for you. "
    }, {
    "id": 34,
    "url": "http://localhost:4000/failure-and-the-future/",
    "title": "Failure and the Future",
    "body": "2012/07/27 - Failures can be debilitating. You got badly injured while skiing and now you do not want to skii again. Your past venture into a startup was a failure and you are scared of starting anything new again. I really wanted to share an excerpt from a book called Sleight of Mouth, that essentially blew my mind off:  My grandfather taught me how to drive. He told me that I could drive safely looking only in the rear view mirror, providing the road ahead is exactly the same as the road behind. So do you assume that the road in front of you is going to be the same as the one before? "
    }, {
    "id": 35,
    "url": "http://localhost:4000/dummies-guide-to-erasure-coding/",
    "title": "Dummies Guide to Erasure Coding",
    "body": "2012/07/01 - If you read this Wikipedia article on erasure coding, you will be more prone to a headache than a person with migraine. Fortunately, there is help. Unfortunately, this article will be from the perspective of storage and not communication. This article is by no means rigorous, but should act as a ‘dummies guide’ and get you started. For example, if you are working on Hadoop and if you hear about people talking about erasure codes for Hadoop, this article will get you started. To begin with we will deal with what are called MDS codes. MDS stands for maximum distance separable. An MDS erasure code is generally represented as (n, k). The basic premise of erasure coding goes as follows:  Take a file and split into k pieces and encode into n pieces. Now, any k pieces can be used to get back the file. So here is the recipe:  Take a file of size M.  Split the file into k chunks, each of the same size M/k.  Now, apply the (n, k) code on these k chunks to get n chunks, each of the same size M/k.  Now the effective size is nM/k. Thus the file is expanded n/k times. We need n to be greater than or equal to k, so n/k is at least 1. If n equals k, you have just split the file and there is no coding performed.  Any k chunks out of the n chunks can be used to get back the file. So this also means that the code can tolerate upto (n - k) erasures. The following figure shows this recipe being followed for a (4, 2) code.  Without really wondering how do actually add files, the following example illustrates one particular case of designing a (4, 2) code.  Erasure codes were first designed to assist in detecting and correcting “problems” when sending data (through an unreliable channel). One of the famous examples of erasure codes are Reed Solomon codes. While erasure codes are also called as error correcting codes, there is a crucial difference between an error and an erasure. If I send ten bits and one bit flipped, an error has occurred, and I do not know where it has occurred. However, if I store ten blocks of a file into different nodes and one node dies, I know exactly which block I lost, and so I know where the erasure has happened. See the difference? So thats all you need to know about erasure coding if you want to get started. Hopefully in a separate post, I will introduce other tools such as Jerasure, which can be used to implement erasure codes. Before signing off, let me now make a case for why it is a great idea to store files as coded blocks instead of complete files in a distributed storage system such as Hadoop. Lets say you have four computers (aka nodes) that you can use to store files. May be you are running Hadoop over these four nodes. Lets say that you have a file A that is two Hadoop-blocks in size. Clearly, instead of putting all the eggs in the same basket, you will want to spread it out and in this case, store the file twice by doing something like this: where X1 is the first block of file A and X2 the second. Another way is to store the coded blocks: Now if you lose nodes 1 and 3, in the case when you store uncoded blocks, X1 is lost permanently, so the file A gets corrupted. Whereas in the case when you store coded blocks, even when those two nodes fail, it is possible to recover X1 and X2 and thus A from A2 and A4. This is because A2 = X2 and A4 = X1 + 2*X2. So one can solve these equations and get back X1 and X2! Neat, isn’t it? One can now extend this to a more generic setting consisting of thousands of computers and may be millions of files (what better name to call other than a ‘data center’?) and see that using erasure codes will help immensely. There are more nuances of the problem that I will not undertake here, and there is something called a repair problem, and if interested, please take a look at Prof. Alex Dimakis’s work. "
    }, {
    "id": 36,
    "url": "http://localhost:4000/reinforcement-learning/",
    "title": "Reinforcement Learning",
    "body": "2012/06/30 - Even though I have used Markov Decision Processes and I know of the bandit problem framework, suddenly I realized that I could not relate these together with reinforcement learning. That meant that there was reading and learning in order. After spending some time on the wiki page and going nowhere, I turned to a tutorial from Petrov at nbu. bg (original link is dead now); and I decided to post what I learnt. The author has given a few good examples, but I wanted to formulate my own so that I can paint a better picture of reinforcement learning and where MDPs and bandit problems fit in the puzzle.  Supervised Learning: Consider a child, to whom you want to teach what is a bicycle using a set of images some of which contain bicycles and some do not. You start showing the child (toddler really) the pictures and tell her whether each image contains a bicycle or not. Eventually the child will learn to identify a bicycle because of your supervision. You have just completed supervised learning, and you could try to do something similar to a computer. Reinforcement Learning: Now suppose you want to teach the child (she is much older now) how to ride a bicycle. One brutal approach is to put her on a bicycle and let her go on her own while merely tell her that she can either turn the handle bar to the left or to the right, or to shift her body to either side to prevent falling. She may fall a few times and recognize what to do and what not to do, and eventually after a few bruises would have learnt how to ride the bicycle. This is an example of reinforcement learning. For more understanding, read the following example from a tutorial from Petrov at nbu. bg (original link is dead now):  To provide the intuition behind reinforcement learning consider the problem of learning to ride a bicycle. The goal given to the RL system is simply to ride the bicycle without falling over. In the first trial, the RLsystem begins riding the bicycle and performs a series of actions that result in the bicycle being tilted 45degrees to the right. At this point their are two actions possible: turn the handle bars left or turn them right. The RL system turns the handle bars to the left and immediately crashes to the ground, thus receiving anegative reinforcement. The RL system has just learned not to turn the handle bars left when tilted 45degrees to the right. In the next trial the RL system performs a series of actions that again result in thebicycle being tilted 45 degrees to the right. The RL system knows not to turn the handle bars to the left, so itperforms the only other possible action: turn right. It immediately crashes to the ground, again receiving astrong negative reinforcement. At this point the RL system has not only learned that turning the handle barsright or left when tilted 45 degrees to the right is bad, but that the “state” of being titled 45 degrees to theright is bad. Again, the RL system begins another trial and performs a series of actions that result in thebicycle being tilted 40 degrees to the right. Two actions are possible: turn right or turn left. The RL systemturns the handle bars left which results in the bicycle being tilted 45 degrees to the right, and ultimatelyresults in a strong negative reinforcement. The RL system has just learned not to turn the handle bars to theleft when titled 40 degrees to the right. By performing enough of these trial-and-error interactions with theenvironment, the RL system will ultimately learn how to prevent the bicycle from ever falling over. So as you can see here, in supervised learning, explicit input is provided whereas this is not the case with reinforcement learning and is a lot harder. Generally, a set of possible actions are specified and a notion of reward (reinforcement) is defined, and the goal is to maximize the cumulative reward by carefully choosing the actions. Falling down gets the child negative reward and probably being able to bike longer gets her positive reward. In a way, there is some input given to reinforcement learning and so it can be considered as a superset of supervised learning. Unsupervised Learning: In case you are wondering whether learning the bicycle is an example of unsupervised learning, it is not. The goal of unsupervised learning is to find hidden patterns and structures in unlabeled data. Example: I give you four numbers {-101, -100, 100, 101}, can you find groups in this data? I am sure your first answer is to find two groups, the set of negative numbers and that of the positive ones. It was easy for you, but how does a computer do it? This is where unsupervised learning comes into pictures. Some of the tools that may be used for unsupervised learning are:  k-means Factor analysis Mixture models Principal Component Analysis Independent Component AnalysisIf you let the child look at the photos may be she will be able to identify that there is a particular object (the bicycle) in a few photos. Here is an excellent lecture by Andrew Ng of Stanford on reinforcement learning: Some of the other examples that Andew Ng states for reinforcement learning:  Training a dog by saying good dog and bad dog.  Training a computer to play chess by assigning positive reward for winning and negative for losing. After many games, the hope is to get the computer win more often.  Training a helicopter by assigning positive reward to fly and negative reward if it crashes. MDPs &amp; Bandits: One way of attacking a reinforcement learning problem is to take random actions at each point of time, completely ignoring the rewards. But in most cases, that is not going to take you anywhere. So in most cases, reinforcement learning problems are formulated as Markov Decision Processes (MDPs). I will talk about it in a bit more detail in a different post. Also, it turns out that the multi-armed bandit problem (again, it will be a separate post) is just a one state MDP where the actions correspond to the arms. The goal is to maximize the expected reward (or minimize the regret) while at each step picking one of the arms. So there you go, I have tried to put together reinforcement learning, MDPs and bandit problems into the same context. If you find anything missing or have any other suggestions, please drop a comment! "
    }, {
    "id": 37,
    "url": "http://localhost:4000/save-an-applet-to-an-image-in-java/",
    "title": "Save an applet to an image in Java",
    "body": "2012/06/27 - Have you ever wanted to save a frame from a Java applet to an image file? In this quick piece of code, I will show you how to save an image produced by an applet to a file. Although it does a different thing, I pretty much wrote this code based on Evo’s reply in StackOverflow. This will be helpful if you want to save a series of frames to images and then animate by making a video, which is what I did to generate this video. I will explain in a later tutorial, how I made that video. package com. smahesh;import java. applet. Applet;import java. awt. Color;import java. awt. Graphics;import java. awt. Graphics2D;import java. awt. GraphicsConfiguration;import java. awt. GraphicsEnvironment;import java. awt. RenderingHints;import java. awt. Transparency;import java. awt. image. BufferedImage;import java. io. File;import java. io. IOException;import javax. imageio. ImageIO;/** * @author Maheswaran Sathiamoorthy * */public class ImageMaker extends Applet{ int circleRadius = 10; private final int MAX_X = 600; private final int MAX_Y = 350; private BufferedImage bufferedImage; private final GraphicsConfiguration gConfig = GraphicsEnvironment   . getLocalGraphicsEnvironment(). getDefaultScreenDevice()   . getDefaultConfiguration(); // If you plan to show on the screen on an applet, apart from saving as an image @Override public void start() {  setSize(MAX_X, MAX_Y);  bufferedImage = create(MAX_X, MAX_Y, true); } // If you plan to show on the screen on an applet, apart from saving as an image @Override public void paint(Graphics g) {  drawCircles(g);  storeImage(); } public void storeImage() {  BufferedImage image = create(MAX_X, MAX_Y, true);  Graphics2D g = image. createGraphics();  // you can disable this if you don't want smooth graphics  g. setRenderingHint(RenderingHints. KEY_ANTIALIASING, RenderingHints. VALUE_ANTIALIAS_ON);  drawCircles(g);  g. dispose();  try {   ImageIO. write(image,  png , new File( /Users/Yourname/Documents/file. png ));  } catch (IOException e) {  } } void drawCircles(Graphics g) {  int count = 0;  for(int r = 10; r &lt;=200; r+=5) {   int x = (int) (MAX_X/2 + r*Math. cos(count*Math. PI/180)) ;   int y = (int) (MAX_Y/2 + r*Math. sin(count*Math. PI/180)) ;   count = count + 20;   g. setColor(Color. BLACK);   g. drawOval(x, y, circleRadius, circleRadius);   g. setColor(new Color((float)235/255, (float)173/255, (float)96/255, 1f));   g. fillOval(x, y, circleRadius, circleRadius);  } } private BufferedImage create(final int width, final int height,   final boolean alpha) {  BufferedImage buffer = gConfig. createCompatibleImage(width, height,       alpha ? Transparency. TRANSLUCENT : Transparency. OPAQUE);  return buffer; }} The output file that was saved by the above program: Thats it! "
    }, {
    "id": 38,
    "url": "http://localhost:4000/the-road-to-wisdom/",
    "title": "The road to wisdom",
    "body": "2012/06/22 - Piet Hein puts it tersely:  The road to wisdom? Well it’s plain and simple to express:Errand errand err againbut lessand lessand less.    Piet Hein, Poet And yes, I have had my share of mistakes. Only time will tell if I am on the correct road! "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        
<div class="container">
	<div class="jumbotron jumbotron-fluid mb-3 pl-0 pt-0 pb-0 bg-white position-relative">
		<div class="h-100 tofront">
			<div class="row justify-content-center">
				<div class="col-md-8 pr-0 pr-md-4 pt-4 pb-4 align-self-center">
					<p class="text-uppercase font-weight-bold">
                        <span class="catlist">
						
                          <a class="sscroll text-danger" href="/mind-over-matter.html">mind over matter</a><span class="sep">, </span>
                        
                        </span>
					</p>
					<h1 class="display-4 mb-4 article-headline">Surely You're Joking, Mr. Feynman!</h1>
					<div class="d-flex align-items-center">
                        
                        <img class="rounded-circle" src="/assets/images/Mahesh-avatar.jpg" alt="Mahesh" width="70"/>
                        
						<small class="ml-3"> Mahesh <span><a target="_blank" href="https://twitter.com/madiator" class="btn btn-outline-success btn-sm btn-round ml-1">Follow</a></span>
                            <span class="text-muted d-block mt-1">Jan 04, 2018 · <span class="reading-time">
  
  
    1 min read
  
</span>
</span>
						</small>
					</div>
				</div>

			</div>
		</div>
	</div>
</div>


<div class="container-lg">
 
</div>

<div class="container-lg pt-4 pb-4">

	       
	<div class="row justify-content-center">
        
        
        <!-- Share -->
		<div class="col-lg-2 pr-4 mb-4 col-md-12">
			<div class="sticky-top sticky-top-offset text-center">
				<div class="text-muted">
					Share this
				</div>
				<div class="share d-inline-block">
					<!-- AddToAny BEGIN -->
					<div class="a2a_kit a2a_kit_size_32 a2a_default_style">
						<a class="a2a_dd" href="https://www.addtoany.com/share"></a>
						<a class="a2a_button_facebook"></a>
						<a class="a2a_button_twitter"></a>
					</div>
					<script async src="https://static.addtoany.com/menu/page.js"></script>
					<!-- AddToAny END -->
				</div>
			</div>
		</div>
        


		<div class="col-md-12 col-lg-8">
            
                            

            <!-- Article -->
			<article class="article-post">                
			<p>I recently finished this book (should have read it long back!) and I must say, what a great read about a great and a curious mind!</p>

<p>Two things stood out for me:</p>

<ol>
  <li>
    <p>How Richard Feynman was able to get into a new field and master it. He chronicles his journey of learning Portuguese (he ends up giving a technical talk entirely in Portuguese in Brazil), playing <em>frigideira</em> (he plays it in the Carnaval in Brazil), and learning painting (he was able to make several portraits and sell some paintings).</p>
  </li>
  <li>
    <p>When he were to understand a new concept as a form of lecture, he would first ask for an example, and at the very beginning ask various questions. As he went along, he would keep working out the details of the problem against the example he got. This allowed him to understand the problem better when it starts to get complicated.</p>
  </li>
</ol>

<p>If you have not heard about the Feynman technique, <a href="https://mattyford.com/blog/2014/1/23/the-feynman-technique-model">check it out</a>!</p>
                
			</article>
			
			<!-- Tags -->
			<div class="mb-4">
				<span class="taglist">
				
				</span>
			</div>
 
            <!-- Mailchimp Subscribe Form -->
            
			<div class="border p-5 bg-lightblue">
				<div class="row justify-content-between">
					<div class="col-md-6 mb-2 mb-md-0">
						<h5 class="font-weight-bold">Join Newsletter</h5>
						 Subscribe to receive updates.
					</div>
					<div class="col-md-6">
						<div class="row">
                            <form action="https://mailchi.mp/a48618289422/subscribe-for-newsletter" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate w-100" target="_blank" novalidate>
                            <div class="mc-field-group">
							
								<input type="email" placeholder="Enter e-mail address" name="EMAIL" class="required email form-control w-100" id="mce-EMAIL" autocomplete="on" required>
							
							
								<button type="submit" value="Subscribe" name="subscribe" class="heart btn btn-success btn-block w-100 mt-2">Subscribe</button>
							
                            </div>
                            </form>
						</div>
					</div>
				</div>
			</div>
            
            
            
             <!-- Author Box -->
                				
				<div class="row mt-5">
					<div class="col-md-2 align-self-center">
                         
                        <img class="rounded-circle" src="/assets/images/Mahesh-avatar.jpg" alt="Mahesh" width="90"/>
                         
					</div>
					<div class="col-md-10">		
                        <h5 class="font-weight-bold">Written by Mahesh <span><a target="_blank" href="https://twitter.com/madiator" class="btn btn-outline-success btn-sm btn-round ml-2">Follow</a></span></h5>
						I think and read about Technology and the Human Mind. I am currently an ML engineer at Google, but the opinions here are my own and do not reflect that of my employer. <a href="/about.html">Read More »</a>					
					</div>

				</div>				
                
            
            <!-- Comments -->
            
                <!--  Don't edit anything here. Set your disqus id in _config.yml -->

<div id="comments" class="mt-5">
    <div id="disqus_thread">
    </div>
    <script type="text/javascript">
        var disqus_shortname = 'mindisblown'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>
    Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
    </noscript>
</div>
            
            
		</div>
        
        
	</div>
</div>


<!-- Aletbar Prev/Next -->
<div class="alertbar">
    <div class="container">
        <div class="row prevnextlinks small font-weight-bold">
          
            <div class="col-md-6 rightborder pl-0">
                <a class="text-dark" href="/new-year-resolutions/"> <img height="30px" class="mr-1" src="https://photos.smugmug.com/Other/Mindisblown/n-ZCNsj/i-BkGxCQv/0/03c66a7a/O/i-BkGxCQv.jpg">  New Year's resolutions</a>
            </div>
          
          
            <div class="col-md-6 text-right pr-0">
                <a class="text-dark" href="/dark-knowledge-and-distillation/"> Dark Knowledge and distillation  <img height="30px" class="ml-1" src="/assets/images/book.jpg"> </a>
            </div>
          
        </div>
    </div>
</div>

    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>


    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>Mahesh Sathiamoorthy</strong></span>
                <span>Copyright © <script>document.write(new Date().getFullYear())</script>.</span>

            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
