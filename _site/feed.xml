<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-12-29T18:50:03-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Mahesh Sathiamoorthy</title><subtitle>Homepage and blog of Mahesh Sathiamoorthy</subtitle><entry><title type="html">2020: It’s a wrap!</title><link href="http://localhost:4000/2020-its-a-wrap/" rel="alternate" type="text/html" title="2020: It’s a wrap!" /><published>2020-12-29T14:00:00-08:00</published><updated>2020-12-29T14:00:00-08:00</updated><id>http://localhost:4000/2020-its-a-wrap</id><content type="html" xml:base="http://localhost:4000/2020-its-a-wrap/">&lt;p&gt;Looking back at my blog, I am disappointed that my last post was in 2018. In fact, my &lt;a href=&quot;/new-year-resolutions&quot;&gt;2018 resolution&lt;/a&gt; was to write 50k public words, but I barely got to a few thousand.
2019 seems to have come and gone, and thanks to a break at work, I have managed to get back to my blog a few days before 2021 knocks on the door.
I do have some good reasons why I didn’t get to my blog all this time (becoming a dad, getting promoted at work, traveling quite a bit, and of course, COVID), but I think the biggest reason is the setup of the blog.&lt;/p&gt;

&lt;p&gt;This site and blog are designed and built using &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;. This means that if I have to write a post, I need to run a few commands and get them pushed to Github. That’s not that bad, but the other reason is that
all of the underlying technologies are installed in my desktop PC, and due to extensive traveling, I was mostly away from the desktop. I love how customizable Jekyll is, but I hate the overhead it has and hate having to debug build errors. If you want to pick up a new hobby, you always have to cut down on hindrances and make it as easy as it is possible to get to it. In my case, the overhead (which wasn’t really that bad, in retrospect) seems to have stopped me from getting to edit posts. In fact, early on, I was using Octopress and in between it broke down, leaving a bad taste in my mouth. All of these, combined with the other complexities of life, meant that I didn’t write as much as I should have written. But thankfully, I did do something else as much as possible in 2019 and 2020: to read as many books as possible. I managed to finish 22 books in 2019 and 32 in 2020 (so almost two books per month).&lt;/p&gt;

&lt;p&gt;Thanks to the year end break, a few days back I took a second look at my site and was appalled at the state of affairs: the homepage was severely outdated (still read as if I was at USC doing my PhD). My homepage lived in a separate repository in Github, and the blog itself was a different repository. They both had different themes, though they maintained the same domain name. I still do have a different domain for the photos I take (which is also sitting unattended for a couple of years now) and hope to bring those into this domain as well.&lt;/p&gt;

&lt;p&gt;I looked into moving over to wordpress so as to make it easy to create, edit and publish posts (remember, no friction), but I still wasn’t happy with the amount of customizability it gave. Fortunately, I stumbled across a different 
&lt;a href=&quot;https://www.wowthemes.net/mundana-jekyll-theme&quot;&gt;template&lt;/a&gt; I liked, and was able to customize it to my liking. I have designed the site such that there are three category of articles: &lt;a href=&quot;/tech.html&quot;&gt;stuff about technology&lt;/a&gt;, which includes tutorials, commentaries etc., and stuff about the mind and cognitition, which I am titling as &lt;a href=&quot;/mind-over-matter.html&quot;&gt;Mind over Matter&lt;/a&gt;, and lastly miscellaneous articles such as this in the form of &lt;a href=&quot;/blog.html&quot;&gt;blogs&lt;/a&gt;.&lt;/p&gt;</content><author><name>mahesh</name></author><category term="blog" /><category term="featured" /><summary type="html">Looking back at my blog, I am disappointed that my last post was in 2018. In fact, my 2018 resolution was to write 50k public words, but I barely got to a few thousand. 2019 seems to have come and gone, and thanks to a break at work, I have managed to get back to my blog a few days before 2021 knocks on the door. I do have some good reasons why I didn’t get to my blog all this time (becoming a dad, getting promoted at work, traveling quite a bit, and of course, COVID), but I think the biggest reason is the setup of the blog.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/buddha.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/buddha.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Finding Passion</title><link href="http://localhost:4000/finding-passion/" rel="alternate" type="text/html" title="Finding Passion" /><published>2018-07-11T22:00:00-07:00</published><updated>2018-07-11T22:00:00-07:00</updated><id>http://localhost:4000/finding-passion</id><content type="html" xml:base="http://localhost:4000/finding-passion/">&lt;p&gt;One would think that Jeff Bezos’s passion has always been Amazon and online retail, but turns out his passions are
Rockets, space travel and propulsion.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=pw7o7ZK12OM&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/pw7o7ZK12OM/0.jpg&quot; alt=&quot;Jeff Bezos talks about his passion&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So he did start out doing what he was NOT passionate about, and was able to make more than a living. I am pretty sure he became passionate about Amazon possibly without actively realizing it, otherwise its unlikely Amazon will be at the place where it is today.&lt;/p&gt;

&lt;p&gt;Bezos thinks that passion finds you, instead of the other way around.
While I think may be he is right about the first half, I think one can still find a passion and become passionate about something that they may not even like to begin with. I think that one can become passionate about anything that they are working on, as long as it is not too boring and allows one to slowly increase expertise in that line of work. You may not be able to become passionate about flipping burgers, but may be able to become passionate about software engineering/machine learning/carpentry or anything else for that matter.&lt;/p&gt;

&lt;p&gt;Let’s look at Bezos’s passion of rockets. Most likely, he was not born with a rocket-gene. He probably became interested in rockets because he either read about rockets, or saw something about rockets on the TV or in real life. He may have seen many things, but why did he become so interested in rockets? There can be any number of reasons here. May be someone influential (that can include father/mother/older sibling) was instrumental, or may be he was exposed to a few other things that made it easy for him to understand about rockets easily. Plus, in the case of rockets, they already bring with them a wow effect that makes it easy for anyone to get captivated with them.&lt;/p&gt;

&lt;p&gt;Sure, his circumstances made it so that the passion of rockets chose him. But the point is that his passion didn’t come out of nowhere, but actually was something that he developed, without realizing it.&lt;/p&gt;

&lt;p&gt;Similarly, it should be possible to develop new passions. You will have to start at the bottom, start exploring the space of the domain that you want to become passionate about, so that you are able to develop good mental representations of the domain and understand the challenges in the domain. It is a slow difficult process, but as you get deeper and deeper into a subject, you will realize that you are more and more interested in it, and will slowly start to develop passion for it.&lt;/p&gt;</content><author><name>mahesh</name></author><category term="mind over matter" /><category term="featured," /><category term="sticky" /><summary type="html">One would think that Jeff Bezos’s passion has always been Amazon and online retail, but turns out his passions are Rockets, space travel and propulsion.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/finding-passion1200.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/finding-passion1200.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deliberate Practice</title><link href="http://localhost:4000/deliberate-practice/" rel="alternate" type="text/html" title="Deliberate Practice" /><published>2018-05-28T11:00:00-07:00</published><updated>2018-05-28T11:00:00-07:00</updated><id>http://localhost:4000/deliberate-practice</id><content type="html" xml:base="http://localhost:4000/deliberate-practice/">&lt;p&gt;I read the book &lt;a href=&quot;https://books.google.com/books/about/Outliers.html?id=3NSImqqnxnkC&quot;&gt;Outliers&lt;/a&gt; several years ago, and as far as I can remember, I was very impressed with the book. Then I started hearing complaints about the book: that the ten thousand hour rule that the author, &lt;a href=&quot;https://en.wikipedia.org/wiki/Malcolm_Gladwell&quot;&gt;Malcom Gladwell&lt;/a&gt;, had been preaching, was over-simplified, and Gladwell may have not stuck to high standards when reporting on some examples.&lt;/p&gt;

&lt;p&gt;In case you have not read that book, and have no idea what I am talking about, here is the gist of ten thousand hours rule: if you want to become a world-class expert in a domain, you will need to practice for around ten thousand hours or so.&lt;/p&gt;

&lt;p&gt;Or at least that’s the message that I remember from the book. The problem is that, this message is not accurate, if not misleading. First, there is no magic number of hours (“ten thousand”) that will buy you greatness and glory, and you cannot just “practice” your way to expertise.&lt;/p&gt;

&lt;p&gt;Here is an example of a &lt;a href=&quot;https://www.goodreads.com/review/show/39454795?book_show_action=true&quot;&gt;random goodreads review&lt;/a&gt; for Outliers that exemplifies the problem with the book:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;He states that it takes approximately 10,000 hours to master something and that gives me comfort.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;“2 hours down, only 9,998 left to go.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ouch.&lt;/p&gt;

&lt;p&gt;The ten thousand hour rule was derived from the work by &lt;a href=&quot;https://en.wikipedia.org/wiki/K._Anders_Ericsson&quot;&gt;Anders Ericsson&lt;/a&gt;, somebody whose job is to study experts, expert performance, and what leads to expertise. After the publication of the Outliers book, Anders started popping up on my radar and was complaining about the book (for example, I heard him talk on &lt;a href=&quot;http://freakonomics.com/podcast/malcolm-gladwell/&quot;&gt;Freakonomics&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;So when I learnt that Anders had published a book on this subject, called &lt;a href=&quot;https://books.google.com/books/about/Peak.html?id=GmcpCgAAQBAJ&quot;&gt;Peak&lt;/a&gt;, I was very curious to take a look. I am still reading the book, but I am glad I picked it up — this book covers a lot of ground and goes the whole nine yards about expertise.&lt;/p&gt;

&lt;p&gt;Running the risk of over-simplification, if there is one main take-away from the book, it is that &lt;em&gt;“deliberate practice”&lt;/em&gt; and not just the regular old “practice” that is essential for expertise. Note that shortly after finishing reading Outliers, I did come across this concept of deliberate practice and understood the subject, but this book has been useful in filling in gaps and connecting a lot of dots that I have been thinking about expertise.&lt;/p&gt;

&lt;h2 id=&quot;lets-talk-about-deliberate-practice&quot;&gt;Let’s talk about deliberate practice&lt;/h2&gt;

&lt;p&gt;Deliberate practice is very different from regular practice.&lt;/p&gt;

&lt;p&gt;Most of us have been typing for a long time (i.e. practicing the skill of “typing”). I might have even practiced this skill for ten thousand hours by now. Am I an expert typist? Heck no. So what happened? I was just practicing it without focusing on improving the skill. It was not mindful, it was very much mindless.&lt;/p&gt;

&lt;p&gt;This sort of mindful practice is what is Ericsson calls as Deliberate practice. You have to intensely focus on what you are doing with a goal to improve. In short, after you come out of a deliberate practice session, your head should hurt (of course I am exaggerating here a bit, but my point is that deliberate practice is very difficult and is not a pleasant activity).&lt;/p&gt;

&lt;p&gt;Ericsson gives similar examples like typing: you don’t know how to play tennis, you start practicing and you slowly get better until one day you realize that you have hit a plateau and have not improved for a long time now. Most likely this is because initially you had to focus quite a bit to get to a decent shape, and then you have stopped focusing, and the games are generally fun. Mostly you are having nice conversations in the court with your opponent.&lt;/p&gt;

&lt;p&gt;This is where focus comes in. You have to focus very carefully to learn from your mistakes.&lt;/p&gt;

&lt;p&gt;Talking about the American swimmer Natalie Coughlin, Ericsson writes:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;While she was a good swimmer, she didn’t become great until she learned to focus throughout her practice.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This struck a chord with me, since, unfortunately, I realized that I have spent a lot of hours in the gym without really focusing on what I was doing.&lt;/p&gt;

&lt;p&gt;So, how exactly does deliberate practice help you become an expert? The answer lies in mental representations.&lt;/p&gt;

&lt;h2 id=&quot;mental-representations&quot;&gt;Mental representations&lt;/h2&gt;

&lt;p&gt;Consider reading: when you started out reading (as a child, I suppose), you would read character by character, and then read out the word. As time went by, you were able to “internalize” words and were able to read out word by word, instead of character by character. What happened here is that your mental represenation of language improved. When you look at a word, you immediately know what the word is, instead of reading it out as i-m-m-e-d-i-a-t-e-l-y.&lt;/p&gt;

&lt;p&gt;Similary, experts when compared to others, have much better mental representations in their domains. For example, Chess players parse the board much differently than the rest of us. When most of us look at the board, we see the individual pieces and the grid and get a rough idea about what the pieces are doing. An absolute new beginner who hasn’t seen a chess board before will not even be able to grasp as much information as you do.
Expert chess players, on the other hand, absorb in a lot more information — they see which pieces are weakly positioned vs. which ones are strongly positioned. They probably almost see a story that is playing out on the board. If you show a valid board, and then ask them to recall, they are able to recall very well. I, on the other hand, will struggle to recall a few pieces here and there. Also, interestingly, if you show an invalid board, with the pieces placed incorrectly, they will struggle to recall the board!&lt;/p&gt;

&lt;p&gt;The point about mental represntations is that better mental representations help you think faster and clearer than others. And deliberate practice helps to hone the mental representations.&lt;/p&gt;

&lt;p&gt;There are of course a number of other factors that help and influence expertise (for example, it helps a lot to have great teachers and to get immediate feedback so as to correct your mistakes), and Ericsson does a great job about explaining all of this.&lt;/p&gt;

&lt;p&gt;If you are interested in delving deeper into this subject, I highly recommend the book!&lt;/p&gt;</content><author><name>mahesh</name></author><category term="mind over matter" /><category term="featured" /><summary type="html">I read the book Outliers several years ago, and as far as I can remember, I was very impressed with the book. Then I started hearing complaints about the book: that the ten thousand hour rule that the author, Malcom Gladwell, had been preaching, was over-simplified, and Gladwell may have not stuck to high standards when reporting on some examples.</summary></entry><entry><title type="html">Dark Knowledge and distillation</title><link href="http://localhost:4000/dark-knowledge-and-distillation/" rel="alternate" type="text/html" title="Dark Knowledge and distillation" /><published>2018-05-27T16:00:00-07:00</published><updated>2018-05-27T16:00:00-07:00</updated><id>http://localhost:4000/dark-knowledge-and-distillation</id><content type="html" xml:base="http://localhost:4000/dark-knowledge-and-distillation/">&lt;p&gt;Guess what this image contains?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/9/9f/Alaskan_Malamute.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most of you will think this is a dog, but a handful may think this is a wolf, and certainly no human readers will guess the above image to contain a car.&lt;/p&gt;

&lt;p&gt;Similarly, if I were a well trained neural network who has never seen this image before, I will assign a very high probability for this being a dog, may be a reasonaly low probability for this being a wolf and extremely low probability for this being a car.&lt;/p&gt;

&lt;p&gt;Now, hold on to this thought. We will come back to this later but first let’s talk about ensembling to improve the performance of machine learning models.&lt;/p&gt;

&lt;h2 id=&quot;ensembling&quot;&gt;Ensembling&lt;/h2&gt;
&lt;p&gt;One way to boost the performance (in terms of accuracy/loss) of a ML system is to ensemble multiple learners instead of a single one. If you are wondering that this is easy and there must be some catch, the catch is that this becomes very costly during serving.&lt;/p&gt;

&lt;p&gt;By serving, I mean using the trained model to run predictions in real world. For example, you have trained an image classifier, and now you want to use it to classify images uploaded from your app. In the absence of ensembling you had to invoke one model, but now with ensembling, you have to compute the predictions across many models and then aggregate them.&lt;/p&gt;

&lt;p&gt;This will add to your CPU cost, to your memory cost, as well as the latency, and so, for many production systems, ensembling remains a difficult choice.&lt;/p&gt;

&lt;p&gt;This where model compression and distillation come in.&lt;/p&gt;

&lt;h2 id=&quot;model-compression&quot;&gt;Model compression&lt;/h2&gt;
&lt;p&gt;Model compression was introduced in KDD 2006 by paper appropriately titled as “Model compression”. The goal is to compress large models into smaller models, so as to reduce the serving costs. At that time, large models generally referred to tree based models (e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;&gt;random forest&lt;/a&gt; that contain several trees). What a different world it was back then!&lt;/p&gt;

&lt;p&gt;Here is a relevant sentence from the paper that summarizes it all:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Compression works by labeling a large unlabeled data set with the target model, and then training a neural net using the newly labeled data&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;http://www.cs.cornell.edu/~caruana/compression.kdd06.pdf&quot;&gt;Bucila et. al.&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;The authors then talk about how to generate this large unlabeled dataset using a few techniques.&lt;/p&gt;

&lt;p&gt;As an aside, note that this paper was written in 2006 when &lt;a href=&quot;https://cloud.google.com/tpu/&quot;&gt;TPUs&lt;/a&gt; didn’t exist. Large tree based models may not be as well tuned to run on custom hardware as neural networks. They may incur a lot of cost when executing “if else” branching control flow statements, whereas with neural networks, we can replace all of these with very fast matrix multiplications.&lt;/p&gt;

&lt;h2 id=&quot;distillation&quot;&gt;Distillation&lt;/h2&gt;
&lt;p&gt;Distillation is very similar to model compression and was introduced recently (2014-2015) by &lt;a href=&quot;https://en.wikipedia.org/wiki/Geoffrey_Hinton&quot;&gt;Geoff Hinton&lt;/a&gt;, father of Deep learning. Distillation refers to “distilling” the knowledge from one model to another, usually with the destination model being smaller than the source model. This sounds very much like model compression except that we want to exploit and learn from the “dark knowledge” that the bigger model has learnt.&lt;/p&gt;

&lt;h2 id=&quot;dark-knowledge&quot;&gt;Dark Knowledge&lt;/h2&gt;

&lt;p&gt;Consider again the image classifier that you want to distill. You want to classify whether an image contains a cow, dog, cat, or a car and you have one big model that does this job fairly well (mostly better than humans), and you want to distill this into a smaller model.&lt;/p&gt;

&lt;p&gt;We have an image with a dog, and your model correctly predicts it to be a dog. If you are training your compressed model using this prediction, it is not very different from training against the targets.&lt;/p&gt;

&lt;p&gt;Instead you could look at the probabilities themselves. But most of these values are almost zero, and so they don’t affect the learning much either (you would have used a cross entropy loss to compare the probabilities of the bigger model against the probabilities of the smaller model).&lt;/p&gt;

&lt;p&gt;The “model compression” paper tries to overcome this problem by instead considering the logits (these are the values of the output layer that are fed to the softmax to get the probabilities). Herein lies the difference between this paper and Hinton’s distillation proposal.&lt;/p&gt;

&lt;p&gt;What Hinton does is to “soften” the probabilities to circumvent the above problem.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://photos.smugmug.com/Other/Mindisblown/n-ZCNsj/i-Hb3Qff8/0/4f5f46d5/L/i-Hb3Qff8-L.png&quot; alt=&quot;slide1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(taken from &lt;a href=&quot;http://www.ttic.edu/dl/dark14.pdf&quot;&gt;http://www.ttic.edu/dl/dark14.pdf&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you look at the second row (“output of geometric ensemble” which you consider as the output of the bigger model), you can notice that the probability for dog is pretty high. But hidden in that row is a lot more knowledge. You can see that the probability for cat is low but not as low as cow/car. This makes sense since there are cases when some dogs may look very similar to cats. Also, you can see that the probability of being a car is thousand times lower than the probability of being a cow. All this knowledge that the model has encoded is what Hinton calls as dark knowledge. So instead of just trying to transfer the knowledge that the image is of dog from a bigger to a smaller model, we want to transfer all this dark knowledge as well. Again, since we are dealing with very small numbers, we can soften these values using a temperature term:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://photos.smugmug.com/Other/Mindisblown/n-ZCNsj/i-xb87SXC/0/87f235f2/O/i-xb87SXC.png&quot; alt=&quot;slide2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(taken from &lt;a href=&quot;http://www.ttic.edu/dl/dark14.pdf&quot;&gt;http://www.ttic.edu/dl/dark14.pdf&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Now when you are training the smaller model, you compare against the soft targets to derive your loss.&lt;/p&gt;</content><author><name>mahesh</name></author><category term="tech" /><summary type="html">Guess what this image contains?</summary></entry><entry><title type="html">Surely You’re Joking, Mr. Feynman!</title><link href="http://localhost:4000/joking-feynman/" rel="alternate" type="text/html" title="Surely You’re Joking, Mr. Feynman!" /><published>2018-01-04T20:30:00-08:00</published><updated>2018-01-04T20:30:00-08:00</updated><id>http://localhost:4000/joking-feynman</id><content type="html" xml:base="http://localhost:4000/joking-feynman/">&lt;p&gt;I recently finished this book (should have read it long back!) and I must say, what a great read about a great and a curious mind!&lt;/p&gt;

&lt;p&gt;Two things stood out for me:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;How Richard Feynman was able to get into a new field and master it. He chronicles his journey of learning Portuguese (he ends up giving a technical talk entirely in Portuguese in Brazil), playing &lt;em&gt;frigideira&lt;/em&gt; (he plays it in the Carnaval in Brazil), and learning painting (he was able to make several portraits and sell some paintings).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When he were to understand a new concept as a form of lecture, he would first ask for an example, and at the very beginning ask various questions. As he went along, he would keep working out the details of the problem against the example he got. This allowed him to understand the problem better when it starts to get complicated.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you have not heard about the Feynman technique, &lt;a href=&quot;https://mattyford.com/blog/2014/1/23/the-feynman-technique-model&quot;&gt;check it out&lt;/a&gt;!&lt;/p&gt;</content><author><name>mahesh</name></author><category term="mind over matter" /><summary type="html">I recently finished this book (should have read it long back!) and I must say, what a great read about a great and a curious mind!</summary></entry><entry><title type="html">New Year’s resolutions</title><link href="http://localhost:4000/new-year-resolutions/" rel="alternate" type="text/html" title="New Year’s resolutions" /><published>2018-01-01T16:00:00-08:00</published><updated>2018-01-01T16:00:00-08:00</updated><id>http://localhost:4000/new-year-resolutions</id><content type="html" xml:base="http://localhost:4000/new-year-resolutions/">&lt;p&gt;2018 is actually here (and that’s bad because it looks like time is crunching through years faster than I would like). And that also means it is time to set new year’s resolutions.&lt;/p&gt;

&lt;p&gt;Image above: Hiking up Koko crater trail in Oahu, Hawaii&lt;/p&gt;

&lt;h3 id=&quot;setting-realistic-goals&quot;&gt;Setting realistic goals&lt;/h3&gt;
&lt;p&gt;For 2017, I had decided that I will do 250 miles of hiking. Given that there are 56 weeks in a year, this leads to a total of 250/56 = 4.46 miles per week. But that should be pretty easy to hit, right? This is where most people make the mistake: they don’t take into account that life comes in between. There are weeks where you may be out of town, or you are sick, or there are a hundred other things that you need to take care of. So realistically, let’s say that I can go out for hikes for only 50% of the weeks. That means I will have to hit 9 miles every time I go for a hike. And since 9 mile hikes sounded reasonable, I decided that 250 miles might be a good goal to keep after all (I realized that I should have also set an elevation goal too, but thankfully my hiking friends made sure that I didn’t always go for low elevation hikes). In fact, this is kind of what happened. According to the spreadsheet I maintain, I did exactly 28 hikes in 2017 (27 hikes got me to the goal of 250 miles). Of course a lot of them were towards the end of the year.&lt;/p&gt;

&lt;p&gt;My friend had set a goal of 25 books to be read last year. How many did he actually do? According to Goodreads, only 4. Sure, he probably forgot to update, but it is very unlikely that he hit 25, especially given that the previous year also he had read less than 10 books. I did a similar mistake once by setting 18 books to read in 2015 and I barely managed to do 12. So for 2017, I had decided to set a smaller and more realistic goal (embarrasingly I didn’t hit my goal; I read only 10 books and didn’t want to count several of the other half read books).&lt;/p&gt;

&lt;p&gt;The point being that one shouldn’t set extremely unrealistic goals (for example, I knew that I can do 10 miles in a single hike, so my hiking goal felt realistic; and since in 2016 I might have done only 50 miles or so of hikes, 250 miles was definintely a worthy pursuit). In fact, I felt that setting goals based on number of books is problematic. I found that towards the end I was trying to search for small books so that I can bump up my numbers faster (and thus I read “Animal farm”). This is why I think instead of setting a goal on number of books that I have read, it is a better idea to set a goal of number of minutes to read. Put another way, it is meaningful to set a goal of reading half and hour everyday.&lt;/p&gt;

&lt;p&gt;I didn’t finish the 12 books that I wanted to read. But I did spend quite a bit of time doing a course (on Neural networks at the beginning of the year), and reading some papers (towards the end of the year; though I need to increase this a lot more), and reading other stuff.&lt;/p&gt;

&lt;h3 id=&quot;my-resolution-for-2018&quot;&gt;My resolution for 2018&lt;/h3&gt;
&lt;p&gt;I know for a fact that my communication skills can use a lot more improvement. While I still need to figure out what I am going to do to improve my verbal communication skill (meetups?), I have decided that I will write as much as possible to improve my written communication skill.&lt;/p&gt;

&lt;p&gt;So there you go. My 2018 resolution is going to be to write. But what good is a goal if it cannot be measured? I can either set a goal of say writing 1 hour every week or set a goal in terms of the number of words written. While, it is tempting to go with the former, I can totally see myself twiddling my thumb for an hour in the pretext of writing something. So I am going to decide a goal based on numbers. I am thinking of writing a total of 50,000 words publicly, and a total of 100,000 words privately (diary etc.). This actually means writing roughly 1000 words publicly every week.&lt;/p&gt;</content><author><name>mahesh</name></author><category term="mind over matter" /><summary type="html">2018 is actually here (and that’s bad because it looks like time is crunching through years faster than I would like). And that also means it is time to set new year’s resolutions.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://photos.smugmug.com/Other/Mindisblown/n-ZCNsj/i-BkGxCQv/0/03c66a7a/O/i-BkGxCQv.jpg" /><media:content medium="image" url="https://photos.smugmug.com/Other/Mindisblown/n-ZCNsj/i-BkGxCQv/0/03c66a7a/O/i-BkGxCQv.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Understanding TensorFlow Graph Execution with a Simple Example</title><link href="http://localhost:4000/understanding-tensorflow-graph/" rel="alternate" type="text/html" title="Understanding TensorFlow Graph Execution with a Simple Example" /><published>2017-07-10T23:00:00-07:00</published><updated>2017-07-10T23:00:00-07:00</updated><id>http://localhost:4000/understanding-tensorflow-graph</id><content type="html" xml:base="http://localhost:4000/understanding-tensorflow-graph/">&lt;p&gt;This post is for absolute beginners. I hope to be able to explain complicated concepts in simple terms to benefit a wider audience (see &lt;a href=&quot;/blog/2012/07/01/dummies-guide-to-erasure-coding/&quot;&gt;dummies guide to erasure coding&lt;/a&gt; post for a different example). Unfortunately though, you will need to know Python and numpy for understanding the code example below.&lt;/p&gt;

&lt;h3 id=&quot;tensorflow&quot;&gt;TensorFlow&lt;/h3&gt;
&lt;p&gt;TensorFlow is a graph based processing framework that is really well suited for building Machine Learning models. I want to show a very simple example that involves TensorFlow’s Variables and Placeholders and illustrate how the graph execution works.&lt;/p&gt;

&lt;p&gt;Note that Tensorflow follows a deferred execution methodology: we initially set up a graph of how the computations should take place and later start the execution. Execution here refers to pumping in data continuously (in batches, more about that later) to the computation graph while tweaking the weights (and of course we generally have a goal in mind while tweaking the weights).&lt;/p&gt;

&lt;h3 id=&quot;simple-example&quot;&gt;Simple example&lt;/h3&gt;
&lt;p&gt;Let’s take a small example to make this idea concrete.&lt;/p&gt;

&lt;p&gt;Let’s say we have a bunch of (x, y) values that satisfy the formula y = ax + b. We know this list of numbers, know the formula, but do not know the values of a and b. The goal is to find out a and b.&lt;/p&gt;

&lt;p&gt;The first step is to define a graph. a and b are the weights that we want to compute. When an input x value is given, we want to compute ax + b, and so this requires a multiplication operator and an addition operator. Therefore we define our graph with these two operators.&lt;/p&gt;

&lt;p&gt;We can randomly assign some values to a and b, and this will give us a concrete value ax + b when a value of x is given to the graph during execution. We want this value to be equal to the corresponding y. So we can compare this value against the y value and use this to tweak the values of a and b. Gradient Descent is generally used to figure out how to modify these values, but for now, we can assume that TensorFlow knows how to do this. We just have to tell it what our ‘loss’ is (some function to compare how worse our estimate ax+b is from y), and we can pick an ‘optimizer’ that does gradient descent. One example of loss would be do take the difference between y and ax+b, and square it.
Strictly speaking, the loss function and the optimizer are picked beforehand when we define the graph.&lt;/p&gt;

&lt;p&gt;Coming back to Variables and Placeholders: In TensorFlow parlance, tf.Variable is used to represent trainable variable, such as weights in a Neural Network. tf.Placeholder is used to represent variables that will be used to feed data to the graph.&lt;/p&gt;

&lt;p&gt;Let’s see the above idea in action.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Set up the data.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_default_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a_weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b_weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x_placeholder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_placeholder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_placeholder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;update_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initialize_all_variables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Launch the tensorflow graph
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rand_indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rand_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rand_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_weight_run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_weight_run&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;y_placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_weight_run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_weight_run&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In the above example, I have set up a dataset corresponding to the points in a straight line y = ax + b. While you can see from the code that the values of a and b are 2 and 4, assume you didn’t have access to the data generation part and you were just given the data.&lt;/p&gt;

&lt;p&gt;Since we want to determine a and b, we define two variables a_weight and b_weight, by using get_variable() of TensorFlow. We then set up placeholders x_placeholder and y_placeholder that will be used to pump data to TensorFlow.
The code pumps 10 randomly selected items of (x, y) each time and TensorFlow tries to determine the weights a_weight and b_weight that minimize the “loss”. Each time, it keeps iterating over its estimate of these weights.&lt;/p&gt;

&lt;p&gt;If everything goes well, the above code should print values that are approximately 2 and 4.&lt;/p&gt;</content><author><name>mahesh</name></author><category term="tech" /><summary type="html">This post is for absolute beginners. I hope to be able to explain complicated concepts in simple terms to benefit a wider audience (see dummies guide to erasure coding post for a different example). Unfortunately though, you will need to know Python and numpy for understanding the code example below.</summary></entry><entry><title type="html">Procrastination Filter</title><link href="http://localhost:4000/procrastination-filter/" rel="alternate" type="text/html" title="Procrastination Filter" /><published>2015-10-14T23:25:00-07:00</published><updated>2015-10-14T23:25:00-07:00</updated><id>http://localhost:4000/procrastination-filter</id><content type="html" xml:base="http://localhost:4000/procrastination-filter/">&lt;p&gt;Have you ever noticed drowning into a sea of articles that you opened via Hacker News? Did you end up buying those nine little things from Amazon of which you don’t use any?  If you think you didn’t, then the very fact that you are reading this article means you have been goofing around the Internet.&lt;/p&gt;

&lt;p&gt;But if you do feel swamped by those 40 tabs on chrome, then I highly urge you to apply this Procrastination Filter to cure your internet inflicted ADD brain. The trick is simple. When you come upon an ‘interesting’ link, tell yourself that you will open it later. Procrastinate it. Chances are that you will forget it, but if it was something important you wouldn’t. So the very act of procrastination acts like a filter separating the gold from the mud. Note that you will not come last in the race if you failed to constantly keep yourself updated (its a rat race anyways).&lt;/p&gt;

&lt;p&gt;Alright, jokes aside, try this next time when you are planning to buy something online: procrastinate.&lt;/p&gt;</content><author><name>mahesh</name></author><category term="mind over matter" /><category term="featured" /><summary type="html">Have you ever noticed drowning into a sea of articles that you opened via Hacker News? Did you end up buying those nine little things from Amazon of which you don’t use any? If you think you didn’t, then the very fact that you are reading this article means you have been goofing around the Internet.</summary></entry><entry><title type="html">Popular Weekend Programming Languages</title><link href="http://localhost:4000/popular-weekend-programming-languages/" rel="alternate" type="text/html" title="Popular Weekend Programming Languages" /><published>2013-05-27T19:04:00-07:00</published><updated>2013-05-27T19:04:00-07:00</updated><id>http://localhost:4000/popular-weekend-programming-languages</id><content type="html" xml:base="http://localhost:4000/popular-weekend-programming-languages/">&lt;p&gt;What are some languages used most often during the weekends? Are there some languages that are inherently more ‘hobbyist’ than others?&lt;/p&gt;

&lt;p&gt;I have attempted to answer these questions for this year’s &lt;a href=&quot;https://github.com/blog/1450-the-github-data-challenge-ii&quot;&gt;GitHub Data Challenge&lt;/a&gt;. &lt;a href=&quot;https://github.com/madiator/githubdatachallenge2&quot;&gt;Here&lt;/a&gt; is my submission.&lt;/p&gt;

&lt;p&gt;One way to answer these questions is to survey thousands of programmers about their language use over weekdays and weekends (which might be fairly difficult, and may not be economically viable). But fortunately for us, GitHub records a swath of data from which such information can be mined. Whenever programmers push code to GitHub, or do other activities such as forking, downloading etc., information is recorded.
The data is available to be downloadable as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;json&lt;/code&gt; files at the &lt;a href=&quot;http://www.githubarchive.org/&quot;&gt;GitHub Archive&lt;/a&gt;, or as a dataset at Google BigQuery. I used the latter.&lt;/p&gt;

&lt;p&gt;One could argue that of all the type of events performed, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PushEvent&lt;/code&gt;s could indicate the language use better than other events (such as WatchEvents). There are definitely several limitations with this approach but nevertheless, let us stick with this metric. The results when all events are used, is shown in subsequent sections.&lt;/p&gt;

&lt;p&gt;The high level overview of what I did is as follows: for each language, the number of pushes during the weekends is counted and is divided by the total number of pushes (for that language) to get a ratio for that language. This ratio will indicate roughly how active these programming languages have been during weekends, and so I used these ratios to rank the languages from the most weekend-oriented to the least weekend-oriented.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/madiator/githubdatachallenge2/raw/master/Images/pushTh1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Here, in order to avoid too many languages, I counted only those languages which had consistently all non-zero number of events every single day.)&lt;/p&gt;

&lt;p&gt;So &lt;a href=&quot;http://www.lua.org/&quot;&gt;Lua&lt;/a&gt;, &lt;a href=&quot;http://dlang.org/&quot;&gt;D&lt;/a&gt; and &lt;a href=&quot;http://arduino.cc/en/Reference/HomePage&quot;&gt;Arduino&lt;/a&gt; seem to be some of the most popular languages during the weekend. Then come Common Lisp, Haskell and Clojure (what a coincidence!). Go also seems to be pretty popular during the weekends. 
I am not too sure what Verilog, and VHDL are doing in the middle! Coffeescript, Actionscript - yes, they make sense. I would have expected Javascript to be around the top (meaning left), but I Javascript is the most popular language (see below) and people are churning out javascript irrespective of the day. Python, Java, PHP are getting relegated to the bottom (right) - these are more for work than hobby. Matlab and R seem to be some of the least frequently used during weekends.&lt;/p&gt;

&lt;p&gt;Next, let us rank this percentage use during weekends for the most popular languages.
To determine language popularity, here are the top-ten languages ranked, determined by counting the number of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PushEvents&lt;/code&gt;  (the number after the language is the raw number of events counted from the dataset).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;JavaScript,8538319&lt;/li&gt;
  &lt;li&gt;Java,5554016&lt;/li&gt;
  &lt;li&gt;Ruby,5032303&lt;/li&gt;
  &lt;li&gt;Python,4285711&lt;/li&gt;
  &lt;li&gt;PHP,4183326&lt;/li&gt;
  &lt;li&gt;C++,2645128&lt;/li&gt;
  &lt;li&gt;C,2620992&lt;/li&gt;
  &lt;li&gt;Shell,1330710&lt;/li&gt;
  &lt;li&gt;C#,978429&lt;/li&gt;
  &lt;li&gt;Perl,917207&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For these languages, the weekend popularity is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/madiator/githubdatachallenge2/raw/master/Images/pushPopular.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that these ratios are not that different, so I would argue that one shouldn’t try to find out why C is before C# etc. But the overall point here is that Perl seems to be used more often during weekends than Java.&lt;/p&gt;

&lt;h3 id=&quot;results-based-on-all-event-types&quot;&gt;Results based on all Event Types&lt;/h3&gt;

&lt;p&gt;Why just count the PushEvents? One could as well include the WatchEvents and ForkEvents, and may be something else. To keep things simple, I did the same analysis by counting all types of events.&lt;/p&gt;

&lt;p&gt;The results are below (again, only those languages are considered which are used everyday).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/madiator/githubdatachallenge2/raw/master/Images/allEventsTh1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most of the pattern followed previously still holds, except now there are a few more languages popping into the diagram.&lt;/p&gt;

&lt;p&gt;And as before, here are top-ten languages counted by aggregating all events (note the difference between this list and the previous list. Objective-C was not there above, but is there in this list; Perl is missing from the above).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;JavaScript,17310652&lt;/li&gt;
  &lt;li&gt;Ruby,9813411&lt;/li&gt;
  &lt;li&gt;Java,9461813&lt;/li&gt;
  &lt;li&gt;Python,8129598&lt;/li&gt;
  &lt;li&gt;PHP,7678254&lt;/li&gt;
  &lt;li&gt;C,5008085&lt;/li&gt;
  &lt;li&gt;C++,4410267&lt;/li&gt;
  &lt;li&gt;Objective-C,2241348&lt;/li&gt;
  &lt;li&gt;Shell,2064431&lt;/li&gt;
  &lt;li&gt;C#,1831029&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The corresponding ranking is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/madiator/githubdatachallenge2/raw/master/Images/allEventsPopular.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The complete list (making sure there have been events for at least half of the total number of days). The results are listed as (language, percentage of events during weekends), and ordered by the percentage in descending order.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Nimrod,35.4411&lt;/li&gt;
  &lt;li&gt;ooc,34.0698&lt;/li&gt;
  &lt;li&gt;Vala,29.9059&lt;/li&gt;
  &lt;li&gt;D,28.4488&lt;/li&gt;
  &lt;li&gt;Dylan,28.041&lt;/li&gt;
  &lt;li&gt;Lua,27.5552&lt;/li&gt;
  &lt;li&gt;Parrot,26.6202&lt;/li&gt;
  &lt;li&gt;Arduino,26.5816&lt;/li&gt;
  &lt;li&gt;Common Lisp,26.4666&lt;/li&gt;
  &lt;li&gt;F#,25.8841&lt;/li&gt;
  &lt;li&gt;Scheme,24.95&lt;/li&gt;
  &lt;li&gt;Verilog,24.8702&lt;/li&gt;
  &lt;li&gt;Io,24.7242&lt;/li&gt;
  &lt;li&gt;Haskell,24.596&lt;/li&gt;
  &lt;li&gt;Delphi,24.5892&lt;/li&gt;
  &lt;li&gt;Pure Data,24.5808&lt;/li&gt;
  &lt;li&gt;Turing,24.4526&lt;/li&gt;
  &lt;li&gt;SuperCollider,24.2344&lt;/li&gt;
  &lt;li&gt;Dart,24.1175&lt;/li&gt;
  &lt;li&gt;Emacs Lisp,24.0541&lt;/li&gt;
  &lt;li&gt;Arc,23.7705&lt;/li&gt;
  &lt;li&gt;Ada,23.5855&lt;/li&gt;
  &lt;li&gt;Visual Basic,23.5394&lt;/li&gt;
  &lt;li&gt;Clojure,23.4217&lt;/li&gt;
  &lt;li&gt;Perl,23.2815&lt;/li&gt;
  &lt;li&gt;DCPU-16 ASM,22.822&lt;/li&gt;
  &lt;li&gt;AppleScript,22.7723&lt;/li&gt;
  &lt;li&gt;Shell,22.7499&lt;/li&gt;
  &lt;li&gt;Julia,22.7151&lt;/li&gt;
  &lt;li&gt;Assembly,22.6994&lt;/li&gt;
  &lt;li&gt;VimL,22.6768&lt;/li&gt;
  &lt;li&gt;Elixir,22.6761&lt;/li&gt;
  &lt;li&gt;Go,22.5093&lt;/li&gt;
  &lt;li&gt;C,22.2897&lt;/li&gt;
  &lt;li&gt;C#,22.084&lt;/li&gt;
  &lt;li&gt;C++,22.0538&lt;/li&gt;
  &lt;li&gt;Smalltalk,22.003&lt;/li&gt;
  &lt;li&gt;HaXe,21.9992&lt;/li&gt;
  &lt;li&gt;OpenEdge ABL,21.9477&lt;/li&gt;
  &lt;li&gt;Gosu,21.8432&lt;/li&gt;
  &lt;li&gt;Standard ML,21.4867&lt;/li&gt;
  &lt;li&gt;CoffeeScript,21.4702&lt;/li&gt;
  &lt;li&gt;Racket,21.1643&lt;/li&gt;
  &lt;li&gt;Boo,21.0025&lt;/li&gt;
  &lt;li&gt;Prolog,20.8579&lt;/li&gt;
  &lt;li&gt;ActionScript,20.5464&lt;/li&gt;
  &lt;li&gt;PHP,20.2422&lt;/li&gt;
  &lt;li&gt;VHDL,20.0203&lt;/li&gt;
  &lt;li&gt;JavaScript,19.9581&lt;/li&gt;
  &lt;li&gt;Objective-C,19.9352&lt;/li&gt;
  &lt;li&gt;Ruby,19.8123&lt;/li&gt;
  &lt;li&gt;Python,19.5424&lt;/li&gt;
  &lt;li&gt;Java,19.4998&lt;/li&gt;
  &lt;li&gt;Logtalk,19.3459&lt;/li&gt;
  &lt;li&gt;Objective-J,19.3309&lt;/li&gt;
  &lt;li&gt;Scala,18.9675&lt;/li&gt;
  &lt;li&gt;Rust,18.9617&lt;/li&gt;
  &lt;li&gt;PowerShell,18.5705&lt;/li&gt;
  &lt;li&gt;Factor,18.0105&lt;/li&gt;
  &lt;li&gt;Coq,17.8117&lt;/li&gt;
  &lt;li&gt;Groovy,17.3457&lt;/li&gt;
  &lt;li&gt;Erlang,17.1946&lt;/li&gt;
  &lt;li&gt;R,17.1551&lt;/li&gt;
  &lt;li&gt;Kotlin,17.0534&lt;/li&gt;
  &lt;li&gt;OCaml,16.8474&lt;/li&gt;
  &lt;li&gt;XQuery,16.7247&lt;/li&gt;
  &lt;li&gt;Rebol,16.4125&lt;/li&gt;
  &lt;li&gt;Scilab,16.0884&lt;/li&gt;
  &lt;li&gt;Matlab,15.9729&lt;/li&gt;
  &lt;li&gt;FORTRAN,15.5981&lt;/li&gt;
  &lt;li&gt;ASP,15.5426&lt;/li&gt;
  &lt;li&gt;Puppet,15.4394&lt;/li&gt;
  &lt;li&gt;AutoHotkey,15.1776&lt;/li&gt;
  &lt;li&gt;Nemerle,14.9266&lt;/li&gt;
  &lt;li&gt;ColdFusion,14.4573&lt;/li&gt;
  &lt;li&gt;Eiffel,14.0177&lt;/li&gt;
  &lt;li&gt;Apex,11.9424&lt;/li&gt;
  &lt;li&gt;Tcl,11.0578&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;workflow&quot;&gt;Workflow&lt;/h3&gt;
&lt;p&gt;Thanks to Google BigQuery, it was a breeze to extract the required information from approximately 60GB of data (of course after many days of tinkering with manually downloading data, figuring out staring at it continuously). But it wasn’t so much of a breeze to download the output so I could further process them. So I have put the csv files in the Data directory. Feel free to use them.&lt;/p&gt;

&lt;p&gt;I used all the data on BigQuery (which started from 11th March 2012 until 8th May 2013, for a total of 424 days).&lt;/p&gt;

&lt;p&gt;The following query lists the number of events per day per language. You can add a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;type='PushEvent'&lt;/code&gt; if you want. The downloaded CVS files are in the data folder of the repository.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;day&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;repository_language&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;day&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;repository_language&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;UTC_USEC_TO_DAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PARSE_UTC_USEC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;created_at&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3600&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;24&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;day&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;githubarchive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;github&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;repository_language&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;day&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;repository_language&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;day&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;conclusion-and-reflections&quot;&gt;Conclusion and Reflections&lt;/h2&gt;
&lt;p&gt;There are indeed several limitations (see below) of this work. The goal here is to not obtain absolute truths, but to try to glean into vast amounts of data and see what it says. Are there some patterns in it, is there something in here that we don’t know? 
I definitely think this work tells us something about the various programming languages. And if anything, it has helped me view programming languages from a different perspective.&lt;/p&gt;

&lt;p&gt;I started out with some other plans:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Compute PageRanks of the various repositories based on the repository fork graph to determine their relative importance. I was mainly interested in doing so since I think there must be a difference between the ordered list of popular repositories (in terms of the number of forks) vs the ordered list of important repositories. For example, Spoon-Knife is the second most forked repository, but its importance should be very low. Getting this graph from BigQuery was very hard and so I had to abandon it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I wanted to see whether news announcements or product launches cause more people to get interested in that language, and therefore more pushes in that language. But from the limited analysis I did, I am afraid I couldn’t find anything like that (may be that tells us something?). For example, &lt;a href=&quot;http://word.bitly.com/post/33232969144/nsq&quot;&gt;Bit.ly announced a realtime distributed message processing system called NSQ on 9th October 2012&lt;/a&gt; and I saw a nice discernible spike in the number of watch events, but I couldn’t see any such trend for the push events. Hopefully I will look more closely into it in the future. Meanwhile, more comments are welcome.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;The data is limited to only those programmers who use GitHub (gasp!).&lt;/li&gt;
  &lt;li&gt;There are definitely many programmers who don’t use GitHub for their work. So all those repositories are not taken into account (which is probably good).&lt;/li&gt;
  &lt;li&gt;GitHub classification of languages is not always accurate (see http://datahackermd.com/2013/language-use-on-github/#comment-798271901).&lt;/li&gt;
  &lt;li&gt;I haven’t accounted for different timezones. While it may not be too difficult to account for it, I think its not worth the effort. Only location information of the programmers are recorded, which can be ambiguous/inaccurate.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;your-comments&quot;&gt;Your comments&lt;/h3&gt;
&lt;p&gt;How would you interpret these results? What else do you think can be done? Let me know your comments!&lt;/p&gt;</content><author><name>mahesh</name></author><category term="tech" /><summary type="html">What are some languages used most often during the weekends? Are there some languages that are inherently more ‘hobbyist’ than others?</summary></entry><entry><title type="html">XORing Elephants: Novel Erasure Codes for Big Data</title><link href="http://localhost:4000/xoring-elephants-novel-erasure-codes-for-big-data/" rel="alternate" type="text/html" title="XORing Elephants: Novel Erasure Codes for Big Data" /><published>2013-04-23T23:43:00-07:00</published><updated>2013-04-23T23:43:00-07:00</updated><id>http://localhost:4000/xoring-elephants-novel-erasure-codes-for-big-data</id><content type="html" xml:base="http://localhost:4000/xoring-elephants-novel-erasure-codes-for-big-data/">&lt;p&gt;This is a post that has been long overdue.&lt;/p&gt;

&lt;p&gt;Our paper, “XORing Elephants: Novel Erasure Codes for Big Data” is now accepted for publication at &lt;a href=&quot;http://www.vldb.org/2013/&quot;&gt;VLDB 2013&lt;/a&gt;. More details about the project, and the paper can be found from the project page: &lt;a href=&quot;http://smahesh.com/HadoopUSC/&quot;&gt;Xorbas Hadoop System&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://photos.smahesh.com/photos/i-kq522gx/0/M/i-kq522gx-M.png&quot; alt=&quot;Xorbas&quot; /&gt;
[Illustration by &lt;a href=&quot;http://users.ece.utexas.edu/~dimakis/&quot;&gt;Dr. Dimakis&lt;/a&gt;]&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;Here is the abstract:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Distributed storage systems for large clusters typically use replication to provide reliability. Recently, erasure codes have been used to reduce the large storage overhead of three-replicated systems. Reed-Solomon codes are the standard design choice and their high repair cost is often considered an unavoidable price to pay for high storage efficiency and high reliability.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;This paper shows how to overcome this limitation. We present a novel family of erasure codes that are efficiently repairable and offer higher reliability compared to Reed-Solomon codes. We show analytically that our codes are optimal on a recently identified tradeoff between locality and minimum distance.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;We implement our new codes in Hadoop HDFS and compare to a currently deployed HDFS module that uses Reed-Solomon codes. Our modified HDFS implementation shows a reduction of approximately 2x on the repair disk I/O and repair network traffic. The disadvantage of the new coding scheme is that it requires 14% more storage compared to Reed-Solomon codes, an overhead shown to be information theoretically optimal to obtain locality. Because the new codes repair failures faster, this provides higher reliability, which is orders of magnitude higher compared to replication.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Please also take a look at my post on &lt;a href=&quot;/dummies-guide-to-erasure-coding&quot;&gt;dummies guide to erasure codes&lt;/a&gt; if you are new to the field.&lt;/p&gt;

&lt;p&gt;As you can read from the abstract, the main problem when using erasure codes is that they generate a lot of repair traffic (whereas replication uses a lot of storage). So our codes try to find a middle-ground between the repair traffic and the storage tradeoff. One thing I want to emphasize here is that, in data centers where there are thousands of servers, failure is the norm rather than the exception. Hopefully I will get to talk a little bit more about failures and the amount of repair traffic generated in data centers in a different post. Stay tuned!&lt;/p&gt;</content><author><name>mahesh</name></author><category term="tech" /><summary type="html">This is a post that has been long overdue.</summary></entry></feed>